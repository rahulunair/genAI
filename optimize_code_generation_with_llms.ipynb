{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Optimize Code Generation with LLMs on IntelÂ® Data Center Max Series GPUs**\n",
    "\n",
    "Hello and welcome! Are you curious about how computers can solve difficult programming tasks and help to streamline the code development process? Do you want to play around with code generation without getting too technical? Then, you've come to the right place.\n",
    "\n",
    "Large language models (LLMs) have a wide range of applications, but they can also be fun to experiment with. Here, we'll use some simple pre-trained models from the [Code Llama](https://huggingface.co/codellama) family of LLMs to explore code generation interactively.\n",
    "\n",
    "Powered by IntelÂ® Data Center GPU Max 1100s, this notebook provides a hands-on experience that doesn't require deep technical knowledge. Whether you are a seasoned developer or learning to code in a new language, this guide is designed for you. \n",
    "\n",
    "Ready to try it out? Let's set up our environment and start exploring the world of code generation with LLMs!\n",
    "\n",
    "Before beginning this tutorial, please ensure you have reviewed the Llama License Agreement at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import sys\n",
    "!echo \"Installation in progress, please wait...\"\n",
    "!{sys.executable} -m pip install accelerate==0.23.0 --no-deps --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install transformers==4.34.0 --no-warn-script-location > /dev/null\n",
    "!echo \"Installation completed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from ipywidgets import VBox, HBox, Button, Dropdown, IntSlider, FloatSlider, Text, Output, Label, Layout\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HTML\n",
    "\n",
    "\n",
    "# random seed\n",
    "if torch.xpu.is_available():\n",
    "    seed = 88\n",
    "    random.seed(seed)\n",
    "    torch.xpu.manual_seed(seed)\n",
    "    torch.xpu.manual_seed_all(seed)\n",
    "\n",
    "def select_device(preferred_device=None):\n",
    "    \"\"\"\n",
    "    Selects the best available XPU device or the preferred device if specified.\n",
    "\n",
    "    Args:\n",
    "        preferred_device (str, optional): Preferred device string (e.g., \"cpu\", \"xpu\", \"xpu:0\", \"xpu:1\", etc.). If None, a random available XPU device will be selected or CPU if no XPU devices are available.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The selected device object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if preferred_device and preferred_device.startswith(\"cpu\"):\n",
    "            print(\"Using CPU.\")\n",
    "            return torch.device(\"cpu\")\n",
    "        if preferred_device and preferred_device.startswith(\"xpu\"):\n",
    "            if preferred_device == \"xpu\" or (\n",
    "                \":\" in preferred_device\n",
    "                and int(preferred_device.split(\":\")[1]) >= torch.xpu.device_count()\n",
    "            ):\n",
    "                preferred_device = (\n",
    "                    None  # Handle as if no preferred device was specified\n",
    "                )\n",
    "            else:\n",
    "                device = torch.device(preferred_device)\n",
    "                if device.type == \"xpu\" and device.index < torch.xpu.device_count():\n",
    "                    vram_used = torch.xpu.memory_allocated(device) / (\n",
    "                        1024**2\n",
    "                    )  # In MB\n",
    "                    print(\n",
    "                        f\"Using preferred device: {device}, VRAM used: {vram_used:.2f} MB\"\n",
    "                    )\n",
    "                    return device\n",
    "\n",
    "        if torch.xpu.is_available():\n",
    "            device_id = random.choice(\n",
    "                range(torch.xpu.device_count())\n",
    "            )  # Select a random available XPU device\n",
    "            device = torch.device(f\"xpu:{device_id}\")\n",
    "            vram_used = torch.xpu.memory_allocated(device) / (1024**2)  # In MB\n",
    "            print(f\"Selected device: {device}, VRAM used: {vram_used:.2f} MB\")\n",
    "            return device\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while selecting the device: {e}\")\n",
    "    print(\"No XPU devices available or preferred device not found. Using CPU.\")\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**A Glimpse Into Code Generation with Language Models**\n",
    "\n",
    "If you're intrigued by how machines can perform a variety of code synthesis and understanding tasks, let's take a closer look at the underlying code. Even if you're not technically inclined, this section will provide a high-level understanding of how it all works:\n",
    "\n",
    "- **Class Definition**: The `CodeChatBot` class is the core of our code-generative AI model. It handles the setup, optimization, and interaction with the LLM.\n",
    "\n",
    "- **Initialization**: When you create an instance of this class, you can specify the model's path, the device to run on (defaulting to Intel's XPU device, if available), and the data type. There's also an option to optimize the model for Intel GPUs using Intel Extension For PyTorch (IPEX).\n",
    "\n",
    "- **Input Preparation**: The `prepare_input` method ensures that the input doesn't exceed the maximum length and combines the previous text with the user input, if required.\n",
    "\n",
    "- **Output Generation**: The `gen_output` method takes the prepared input and several parameters controlling the generation process, like temperature, top_p, top_k, etc., and produces the code.\n",
    "\n",
    "- **Warm-up**: Before the main interactions, the `warmup_model` method helps in \"warming up\" the model to make subsequent runs faster.\n",
    "\n",
    "- **Code Processing**: The `unique_sentences` method handles the code processing to ensure the generated code is readable and free from repetitions or unnecessary echoes.\n",
    "\n",
    "Feel free to explore the code and play around with different parameters. Remember, this is a simple and interactive way to experiment with code generation. It's not a cutting-edge chatbot, but rather a playful tool to engage with language models. Enjoy the journey into the world of LLMs, using IntelÂ® Data Center GPU Max 1100s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CACHE_PATH = \"/home/common/data/Big_Data/GenAI/llm_models\"\n",
    "class CodeChatBot:\n",
    "    \"\"\"\n",
    "    CodeChatBot is a class for generating responses based on programming-specific prompts using a pretrained LLM.\n",
    "\n",
    "    Attributes:\n",
    "    - device: The device to run the model on. Default is \"xpu\" if available, otherwise \"cpu\".\n",
    "    - model: The loaded model for code generation.\n",
    "    - tokenizer: The loaded tokenizer for the model.\n",
    "    - torch_dtype: The data type to use in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path: str = \"codellama/CodeLlama-7b-hf\",\n",
    "        torch_dtype: torch.dtype = torch.bfloat16,\n",
    "        optimize: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        The initializer for CodeChatBot class.\n",
    "\n",
    "        Parameters:\n",
    "        - model_id_or_path: The identifier or path of the pretrained model.\n",
    "        - torch_dtype: The data type to use in the model. Default is torch.bfloat16.\n",
    "        - optimize: If True, ipex is used to optimized the model\n",
    "        \"\"\"\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.device = select_device(\"xpu\")\n",
    "        self.model_id_or_path = model_id_or_path\n",
    "        local_model_id = self.model_id_or_path.replace(\"/\", \"--\")\n",
    "        local_model_path = os.path.join(MODEL_CACHE_PATH, local_model_id)\n",
    "        \n",
    "        if (\n",
    "            self.device == self.device.startswith(\"xpu\")\n",
    "            if isinstance(self.device, str)\n",
    "            else self.device.type == \"xpu\"\n",
    "        ):\n",
    "\n",
    "            self.autocast = torch.xpu.amp.autocast\n",
    "        else:\n",
    "            self.autocast = torch.cpu.amp.autocast\n",
    "        self.torch_dtype = torch_dtype\n",
    "\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                local_model_path, trust_remote_code=True, \n",
    "            )\n",
    "            self.model = (\n",
    "                AutoModelForCausalLM.from_pretrained(\n",
    "                    local_model_path,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=self.torch_dtype,\n",
    "                )\n",
    "                .to(self.device)\n",
    "                .eval()\n",
    "            )\n",
    "        except (OSError, ValueError, EnvironmentError) as e:\n",
    "            logging.info(\n",
    "                f\"Tokenizer / model not found locally. Downloading tokenizer / model for {self.model_id_or_path} to cache...: {e}\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_id_or_path, trust_remote_code=True, \n",
    "            )\n",
    "            self.model = (\n",
    "                AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_id_or_path,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=self.torch_dtype,\n",
    "                )\n",
    "                .to(self.device)\n",
    "                .eval()\n",
    "            )\n",
    "            \n",
    "        self.max_length = 256\n",
    "\n",
    "        if optimize:\n",
    "            if hasattr(ipex, \"optimize_transformers\"):\n",
    "                try:\n",
    "                    ipex.optimize_transformers(self.model, dtype=self.torch_dtype)\n",
    "                except:\n",
    "                    ipex.optimize(self.model, dtype=self.torch_dtype)\n",
    "            else:\n",
    "                ipex.optimize(self.model, dtype=self.torch_dtype)\n",
    "\n",
    "    def prepare_input(self, previous_text, user_input):\n",
    "        \"\"\"Prepare the input for the model, ensuring it doesn't exceed the maximum length.\"\"\"\n",
    "        response_buffer = 100\n",
    "        user_input = (\n",
    "             \"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{user_input}\\n\\n### Response:\")\n",
    "        combined_text = previous_text + \"\\nUser: \" + user_input + \"\\nBot: \"\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            combined_text, return_tensors=\"pt\", truncation=False\n",
    "        )\n",
    "        adjusted_max_length = self.max_length - response_buffer\n",
    "        if input_ids.shape[1] > adjusted_max_length:\n",
    "            input_ids = input_ids[:, -adjusted_max_length:]\n",
    "        return input_ids.to(device=self.device)\n",
    "\n",
    "    def gen_output(\n",
    "        self, input_ids, temperature, top_p, top_k, num_beams, repetition_penalty\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate the output based on the given input IDs and generation parameters.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): The input tensor containing token IDs.\n",
    "            temperature (float): The temperature for controlling randomness in Boltzmann distribution.\n",
    "                                Higher values increase randomness, lower values make the generation more deterministic.\n",
    "            top_p (float): The cumulative distribution function (CDF) threshold for Nucleus Sampling.\n",
    "                           Helps in controlling the trade-off between randomness and diversity.\n",
    "            top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "            num_beams (int): The number of beams for beam search. Controls the breadth of the search.\n",
    "            repetition_penalty (float): The penalty applied for repeating tokens.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The generated output tensor.\n",
    "        \"\"\"\n",
    "        print(f\"Using max length: {self.max_length}\")\n",
    "        with self.autocast(\n",
    "            enabled=True if self.torch_dtype != torch.float32 else False,\n",
    "            dtype=self.torch_dtype,\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                output = self.model.generate(\n",
    "                    input_ids,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    max_length=self.max_length,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    num_beams=num_beams,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                )\n",
    "                return output\n",
    "\n",
    "    def warmup_model(\n",
    "        self, temperature, top_p, top_k, num_beams, repetition_penalty\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Warms up the model by generating a sample response.\n",
    "        \"\"\"\n",
    "        sample_prompt = \"\"\"A dialog, where User interacts with a helpful Bot.\n",
    "        AI is helpful, kind, obedient, honest, and knows its own limits.\n",
    "        User: Hello, Bot.\n",
    "        Bot: Hello! How can I assist you today?\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer(sample_prompt, return_tensors=\"pt\").input_ids.to(\n",
    "            device=self.device\n",
    "        )\n",
    "        _ = self.gen_output(\n",
    "            input_ids,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "        )\n",
    "\n",
    "    def strip_response(self, generated_code):\n",
    "        \"\"\"Remove ### Response: from string if exists.\"\"\"\n",
    "        match = re.search(r'### Response:(.*)', generated_code, re.S)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "        else:\n",
    "            return generated_code\n",
    "        \n",
    "    def unique_sentences(self, text: str) -> str:\n",
    "        sentences = text.split(\". \")\n",
    "        if sentences[-1] and sentences[-1][-1] != \".\":\n",
    "            sentences = sentences[:-1]\n",
    "        sentences = set(sentences)\n",
    "        return \". \".join(sentences) + \".\" if sentences else \"\"\n",
    "\n",
    "    def interact(\n",
    "        self,\n",
    "        out: Output,  # Output widget to display the conversation\n",
    "        with_context: bool = True,\n",
    "        temperature: float = 0.10,\n",
    "        top_p: float = 0.95,\n",
    "        top_k: int = 20,\n",
    "        num_beams: int = 3,\n",
    "        repetition_penalty: float = 1.80,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Handle the chat loop where the user provides input and receives a model-generated response.\n",
    "\n",
    "        Args:\n",
    "            with_context (bool): Whether to consider previous interactions in the session. Default is True.\n",
    "            temperature (float): The temperature for controlling randomness in Boltzmann distribution.\n",
    "                                 Higher values increase randomness, lower values make the generation more deterministic.\n",
    "            top_p (float): The cumulative distribution function (CDF) threshold for Nucleus Sampling.\n",
    "                           Helps in controlling the trade-off between randomness and diversity.\n",
    "            top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "            num_beams (int): The number of beams for beam search. Controls the breadth of the search.\n",
    "            repetition_penalty (float): The penalty applied for repeating tokens.\n",
    "            \"\"\"\n",
    "        previous_text = \"\"\n",
    "    \n",
    "        def display_user_input_widgets():\n",
    "            default_color = \"\\033[0m\"\n",
    "            user_color, user_icon = \"\\033[94m\", \"ðŸ˜€ \"\n",
    "            bot_color, bot_icon = \"\\033[92m\", \"ðŸ¤– \"\n",
    "            user_input_widget = Text(placeholder=\"Type your message here...\", layout=Layout(width='80%'))\n",
    "            send_button = Button(description=\"Send\", button_style = \"primary\", layout=Layout(width='10%'))\n",
    "            chat_spin = HTML(value = \"\")\n",
    "            spin_style = \"\"\"\n",
    "            <div class=\"loader\"></div>\n",
    "            <style>\n",
    "            .loader {\n",
    "              border: 5px solid #f3f3f3;\n",
    "              border-radius: 50%;\n",
    "              border-top: 5px solid #3498db;\n",
    "              width: 8px;\n",
    "              height: 8px;\n",
    "              animation: spin 3s linear infinite;\n",
    "            }\n",
    "            @keyframes spin {\n",
    "              0% { transform: rotate(0deg); }\n",
    "              100% { transform: rotate(360deg); }\n",
    "            }\n",
    "            </style>\n",
    "            \"\"\"\n",
    "            display(HBox([chat_spin, user_input_widget, send_button, ]))\n",
    "            \n",
    "            def on_send(button):\n",
    "                nonlocal previous_text\n",
    "                send_button.button_style = \"warning\"\n",
    "                chat_spin.value = spin_style\n",
    "                orig_input = \"\"\n",
    "                user_input = user_input_widget.value\n",
    "                with out:\n",
    "                    print(f\" {user_color}{user_icon}You: {user_input}{default_color}\")\n",
    "                if user_input.lower() == \"exit\":\n",
    "                    return\n",
    "                if with_context:\n",
    "                    self.max_length = 256\n",
    "                    input_ids = self.prepare_input(previous_text, user_input)\n",
    "                else:\n",
    "                    self.max_length = 96\n",
    "                    input_ids = self.tokenizer.encode(user_input, return_tensors=\"pt\").to(self.device)\n",
    "    \n",
    "                output_ids = self.gen_output(\n",
    "                    input_ids,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    num_beams=num_beams,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                )\n",
    "                generated_code = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                generated_code = self.strip_response(generated_code)\n",
    "                send_button.button_style = \"success\"\n",
    "                chat_spin.value = \"\"\n",
    "\n",
    "                with out:\n",
    "                    if orig_input:\n",
    "                        user_input = orig_input\n",
    "                    print(f\" {bot_color}{bot_icon}Bot: {generated_code}{default_color}\")    \n",
    "                if with_context:\n",
    "                    previous_text += \"\\nUser: \" + user_input + \"\\nBot: \" + generated_code\n",
    "                user_input_widget.value = \"\" \n",
    "                display_user_input_widgets()\n",
    "            send_button.on_click(on_send)\n",
    "        display_user_input_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Setting Up the Interactive Code Generation Interface**\n",
    "\n",
    "In the next section, we'll create an interactive code generation interface right here in this notebook. This will enable you to select a model, provide a prompt, and adjust various parameters without touching the code itself.\n",
    "\n",
    "- **Model Selection**: Choose from the following pre-trained Code Llama models:\n",
    "    - `Code Llama`: The foundational model for general-purpose programming tasks and code completion.\n",
    "    - `Code Llama - Python`: This model is specialized for Python code generation.\n",
    "    - `Code Llama - Instruct`: This model is specialized to understand and follow natural language instructions. \n",
    "- **Interaction Mode**: Decide whether to interact with or without context, allowing the model to remember previous interactions or treat each input independently.\n",
    "- **Temperature**: Adjust this parameter to control the randomness in code generation. Higher values increase creativity; lower values make the generation more deterministic.\n",
    "- **Top_p**, **Top_k**: Play with these parameters to influence the diversity and quality of the generated code.\n",
    "- **Number of Beams**: Control the breadth of the search in code generation.\n",
    "- **Repetition Penalty**: Modify this to prevent or allow repeated phrases and sentences.\n",
    "\n",
    "Once you've set your preferences, you can start the interaction and even reset or reload the model to try different settings. Let's set this up and explore the playful world of code generation using IntelÂ® Data Center GPU Max 1100s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cache = {}\n",
    "\n",
    "from ipywidgets import HTML\n",
    "def code_generation_with_llm():\n",
    "    models = [\"Code Llama\", \"Code Llama - Python\", \"Code Llama - Instruct\"]\n",
    "    interaction_modes = [\"Interact with context\", \"Interact without context\"]\n",
    "    model_dropdown = Dropdown(options=models, value=models[0], description=\"Model:\")\n",
    "    interaction_mode = Dropdown(options=interaction_modes, value=interaction_modes[1], description=\"Interaction:\")\n",
    "    temperature_slider = FloatSlider(value=0.71, min=0, max=1, step=0.01, description=\"Temperature:\")\n",
    "    top_p_slider = FloatSlider(value=0.95, min=0, max=1, step=0.01, description=\"Top P:\")\n",
    "    top_k_slider = IntSlider(value=40, min=0, max=100, step=1, description=\"Top K:\")\n",
    "    num_beams_slider = IntSlider(value=3, min=1, max=10, step=1, description=\"Num Beams:\")\n",
    "    repetition_penalty_slider = FloatSlider(value=1.80, min=0, max=2, step=0.1, description=\"Rep Penalty:\")\n",
    "    \n",
    "    out = Output()    \n",
    "    left_panel = VBox([model_dropdown, interaction_mode], layout=Layout(margin=\"0px 20px 10px 0px\"))\n",
    "    right_panel = VBox([temperature_slider, top_p_slider, top_k_slider, num_beams_slider, repetition_penalty_slider],\n",
    "                       layout=Layout(margin=\"0px 0px 10px 20px\"))\n",
    "    user_input_widgets = HBox([left_panel, right_panel], layout=Layout(margin=\"0px 50px 10px 0px\"))\n",
    "    spinner = HTML(value=\"\")\n",
    "    start_button = Button(description=\"Start Interaction!\", button_style=\"primary\")\n",
    "    start_button_spinner = HBox([start_button, spinner])\n",
    "    start_button_spinner.layout.margin = '0 auto'\n",
    "    display(user_input_widgets)\n",
    "    display(start_button_spinner)\n",
    "    display(out)\n",
    "    \n",
    "    def on_start(button):\n",
    "        start_button.button_style = \"warning\"\n",
    "        start_button.description = \"Loading...\"\n",
    "        spinner.value = \"\"\"\n",
    "        <div class=\"loader\"></div>\n",
    "        <style>\n",
    "        .loader {\n",
    "          border: 5px solid #f3f3f3;\n",
    "          border-radius: 50%;\n",
    "          border-top: 5px solid #3498db;\n",
    "          width: 16px;\n",
    "          height: 16px;\n",
    "          animation: spin 3s linear infinite;\n",
    "        }\n",
    "        @keyframes spin {\n",
    "          0% { transform: rotate(0deg); }\n",
    "          100% { transform: rotate(360deg); }\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\"\n",
    "        out.clear_output()\n",
    "        with out:\n",
    "            print(\"\\nSetting up the model, please wait...\")\n",
    "            \n",
    "        if model_dropdown.value == \"Code Llama - Python\":\n",
    "            model_choice = \"codellama/CodeLlama-7b-Python-hf\"\n",
    "        elif model_dropdown.value == \"Code Llama - Instruct\":\n",
    "            model_choice = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "        else:\n",
    "            model_choice = \"codellama/CodeLlama-7b-hf\"\n",
    "        with_context = interaction_mode.value == interaction_modes[0]\n",
    "        temperature = temperature_slider.value\n",
    "        top_p = top_p_slider.value\n",
    "        top_k = top_k_slider.value\n",
    "        num_beams = num_beams_slider.value\n",
    "        repetition_penalty = repetition_penalty_slider.value\n",
    "        model_key = (model_choice, \"xpu\")\n",
    "        if model_key not in model_cache:\n",
    "            model_cache[model_key] = CodeChatBot(model_id_or_path=model_choice)\n",
    "        bot = model_cache[model_key]\n",
    "        \n",
    "        with out:\n",
    "            start_button.button_style = \"success\"\n",
    "            start_button.description = \"Refresh\"\n",
    "            spinner.value = \"\"\n",
    "            print(\"Ready!\")\n",
    "            print(\"\\nNote: This is a demonstration using pretrained models which were not fine-tuned for chat.\")\n",
    "            print(\"If the bot doesn't respond, try clicking on refresh.\\n\")\n",
    "            print(\"\\nEnter a coding-related prompt or query below.\")\n",
    "        try:\n",
    "            with out:\n",
    "                bot.interact(\n",
    "                    with_context=with_context,\n",
    "                    out=out,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    num_beams=num_beams,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            with out:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    start_button.on_click(on_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Let's Dive In and Have Some Fun with Code-Generative LLMs!**\n",
    "\n",
    "Ready for a playful interaction with some interesting LLMs? The interface below lets you choose from different models and settings. Just select your preferences, click the \"Start Interaction!\" button, and you're ready to chat.\n",
    "\n",
    "You can ask questions, make statements, or simply explore how the model responds to different inputs. It's a friendly way to get acquainted with AI and see what it has to say.\n",
    "\n",
    "Remember, this is all in good fun, and the models are here to engage with you. So go ahead, start a conversation, and enjoy the interaction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Dropdown(description='Model:', options=('Code Llama', 'Code Llama - Python', 'Coâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='primary', description='Start Interaction!', style=ButtonStyle()), HTML(valâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_generation_with_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models Disclaimer and Information\n",
    "\n",
    "### Model cards:\n",
    "- **Code Llama:** [CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf)\n",
    "- **Code Llama - Python:** [CodeLlama-7b-Python-hf](https://huggingface.co/codellama/CodeLlama-7b-Python-hf)\n",
    "- **Code Llama - Instruct:** [CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf)\n",
    "\n",
    "### Code Llama License:\n",
    "A custom commercial license is available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\n",
    "\n",
    "### Additional Resources:\n",
    "- More information on Code Llama can be found in the paper [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) or it's [arXiv](https://arxiv.org/abs/2308.12950) page.\n",
    "- Review the Llama Responsible Use Guide available at: https://ai.meta.com/llama/responsible-use-guide/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notices and Disclaimers\n",
    "\n",
    "Please be aware that while LLMs like Code Llama are powerful tools for code generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It is advisable to carefully review the generated output and consider the context and application in which you are using these models. Usage of these models must also adhere to their licensing agreements and be in accordance with ethical guidelines and best practices for AI. \n",
    "\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intelâ€™s provision of these resources does not expand or otherwise alter Intelâ€™s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
