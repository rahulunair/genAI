{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ba5869dc-648c-4183-8963-7652819462e1",
   "metadata": {},
   "source": [
    "--- created by Rajashekar Kasturi\n",
    "--- This is completely inspired from the Orginal work of Text_to_Image notebook from Rahul Nair in (https://github.com/rahulunair/genAI)\n",
    "--- Environment: Intel® Tiber™ Developer (https://console.cloud.intel.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f57b36-16a0-4908-bc09-e09af13bcaa3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Welcome to the Magic of Words! - Text-to-Image with Stable Diffusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95841e46-5430-4cb7-9733-de863e2ae321",
   "metadata": {},
   "source": [
    "Welcome! Whether you're a writer, designer, developer, or someone curious about the intersection of text and visuals, this guide is for you.\n",
    "\n",
    "Have you ever wanted to convert a description or phrase into an image? With Text-to-Image Stable Diffusion, you can do just that. Input a description like \"a quiet sunset over the ocean\" or \"a cityscape at night,\" and see it translated into a visual representation.\n",
    "\n",
    "This notebook is powered by Intel® Data Center GPU Max Series and will walk you through examples, help you understand the outputs, and give pointers on how to get the best results. No technical deep dives, just clear steps and explanations.\n",
    "\n",
    "Ready to turn your words into visuals?, Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eae7cc-beef-43ca-adb1-43fee79c1924",
   "metadata": {},
   "source": [
    "#### Let's get started by setting up few things and importing the required python packages!\n",
    "\n",
    "**Setup**\n",
    "1. Uncomment below cell and run to create new conda environment.\n",
    "2. To select new environment/kernel in Jupyter, select \"text_to_image\" in top-right dropdown of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07740ca8-7e1c-4759-b5d4-d1b86fde54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export ENV_NAME=\"text_to_image_env\"\n",
    "export PATH=/opt/intel/miniforge3/condabin:$PATH # could add this as default on ITDC Jupyter bash.\n",
    "\n",
    "conda create -n $ENV_NAME python=3.10 -y  # Python version\n",
    "\n",
    "# Dependencies\n",
    "conda run -n $ENV_NAME pip install\\\n",
    " numpy==1.26.4\\\n",
    " importlib-metadata==8.5.0\\\n",
    " accelerate==0.23.0\\\n",
    " transformers==4.38.0\\\n",
    " matplotlib==3.9.2\\\n",
    " validators==0.22.0\\\n",
    " ipykernel==6.29.5\\\n",
    " ipywidgets==8.1.5\n",
    "\n",
    "# XPU dependencies\n",
    "conda run -n $ENV_NAME pip install\\\n",
    " torch==2.1.0.post2\\\n",
    " torchvision==0.16.0.post2\\\n",
    " torchaudio==2.1.0.post2\\\n",
    " oneccl_bind_pt==2.1.300+xpu\\\n",
    " intel-extension-for-pytorch==2.1.30+xpu  --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\\\n",
    "\n",
    "# To make environment available to Jupyter Kernel. Please select in dropdown (top right dropdown)\n",
    "conda run -n $ENV_NAME python -m ipykernel install --user --name=$ENV_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b634eb-da9f-482c-aa0a-1af84f736d22",
   "metadata": {},
   "source": [
    "**Now select the kerenel as 'Diffusion environment' in the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25d6c0-1c17-4497-bd74-f7795887d7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Suppress warnings for a cleaner output.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import intel_extension_for_pytorch as ipex  # Used for optimizing PyTorch models\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import StableDiffusion3Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22619ef2-118d-41b0-ac46-9c80261cc138",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A Glimpse Behind the Scenes**\n",
    "\n",
    "For those intrigued by the underpinnings of this adventure, let's delve into the technicalities of the code. No worries if you're not aiming for a deep dive; understanding this isn't a prerequisite to run the notebook. But for the tech-curious, let's dissect:\n",
    "\n",
    "- **Class**: At the heart of our operations is the `Text2ImgModel` class. This class defines the process of transforming textual prompts into visual masterpieces.\n",
    "\n",
    "- **Pipeline Loading**: The `_load_pipeline` function is where we bring the pre-trained model onboard. This sets the stage for all our text-to-image transformations.\n",
    "\n",
    "- **Optimization**: Performance is paramount. With the _optimize_pipeline and optimize_pipeline methods, we leverage Intel-specific optimizations using the Intel Extension For PyTorch (IPEX) to ensure our model runs fast!.\n",
    "\n",
    "- **The Grand Finale - Image Generation**: The `generate_images` method is where dreams meet reality. It interprets the textual prompts, consults with the model, and then crafts images that encapsulate the essence of the prompts. You can choose the model, provide a prompt and specify the number of images.\n",
    "\n",
    "Intrigued? Dive into the code below and see how we've tailored the image generation pipeline, ensuring it's optimized for Intel GPUs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de4c9a-5a9e-4c95-96cb-8b4280c8fa04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Text2ImgModel:\n",
    "    \"\"\"\n",
    "    Text2ImgModel is a class for generating images based on text prompts using a pretrained model.\n",
    "\n",
    "    Attributes:\n",
    "    - device: The device to run the model on. Default to \"xpu\" - Intel dGPUs.\n",
    "    - pipeline: The loaded model pipeline.\n",
    "    - data_type: The data type to use in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path: str,\n",
    "        device: str = \"xpu\",\n",
    "        torch_dtype: torch.dtype = torch.bfloat16,\n",
    "        optimize: bool = True,\n",
    "        warmup: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        The initializer for Text2ImgModel class.\n",
    "\n",
    "        Parameters:\n",
    "        - model_id_or_path: The identifier or path of the pretrained model.\n",
    "        - device: The device to run the model on. Default is \"xpu\".\n",
    "        - torch_dtype: The data type to use in the model. Default is torch.bfloat16.\n",
    "        - optimize: Whether to optimize the model after loading. Default is True.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "        self.pipeline = self._load_pipeline(\n",
    "            model_id_or_path, torch_dtype,\n",
    "        )\n",
    "        self.data_type = torch_dtype\n",
    "        if optimize:\n",
    "            start_time = time.time()\n",
    "            # print(\"Optimizing the model...\")\n",
    "            self.optimize_pipeline()\n",
    "            # print(\n",
    "            #    \"Optimization completed in {:.2f} seconds.\".format(\n",
    "            #        time.time() - start_time\n",
    "            #    )\n",
    "            # )\n",
    "        if warmup:\n",
    "            self.warmup_model()\n",
    "\n",
    "    def _optimize_pipeline(self, pipeline: StableDiffusion3Pipeline) -> StableDiffusion3Pipeline:\n",
    "        \"\"\"\n",
    "        Optimizes the model for inference using ipex.\n",
    "\n",
    "        Parameters:\n",
    "        - pipeline: The model pipeline to be optimized.\n",
    "\n",
    "        Returns:\n",
    "        - pipeline: The optimized model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        for attr in dir(pipeline):\n",
    "            try:\n",
    "                if isinstance(getattr(pipeline, attr), nn.Module):\n",
    "                    setattr(\n",
    "                        pipeline,\n",
    "                        attr,\n",
    "                        ipex.optimize(\n",
    "                            getattr(pipeline, attr).eval(),\n",
    "                            dtype=pipeline.text_encoder.dtype,\n",
    "                            inplace=True,\n",
    "                        ),\n",
    "                    )\n",
    "            except AttributeError:\n",
    "                pass\n",
    "        return pipeline\n",
    "\n",
    "    def _load_pipeline(\n",
    "        self,\n",
    "        model_id_or_path: str,\n",
    "        torch_dtype: torch.dtype,\n",
    "    ) -> StableDiffusion3Pipeline:\n",
    "        \"\"\"\n",
    "        Loads the pretrained model and prepares it for inference.\n",
    "\n",
    "        Parameters:\n",
    "        - model_id_or_path: The identifier or path of the pretrained model.\n",
    "        - torch_dtype: The data type to use in the model.\n",
    "\n",
    "        Returns:\n",
    "        - pipeline: The loaded model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Loading the model...\")\n",
    "\n",
    "        load_path = model_id_or_path\n",
    "        \n",
    "        pipeline = StableDiffusion3Pipeline.from_pretrained(\n",
    "            load_path,\n",
    "            torch_dtype=torch_dtype,\n",
    "        )\n",
    "        pipeline = pipeline.to(self.device)\n",
    "        #print(\"Model loaded.\")\n",
    "        return pipeline\n",
    "\n",
    "    def warmup_model(self):\n",
    "        \"\"\"\n",
    "        Warms up the model by generating a sample image.\n",
    "        \"\"\"\n",
    "        print(\"Setting up model...\")\n",
    "        start_time = time.time()\n",
    "        self.generate_images(\n",
    "            prompt=\"A beautiful sunset over the mountains\",\n",
    "            num_images=1,\n",
    "            save_path=\".tmp\",\n",
    "        )\n",
    "        print(\n",
    "            \"Model is set up and ready! Warm-up completed in {:.2f} seconds.\".format(\n",
    "                time.time() - start_time\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def optimize_pipeline(self) -> None:\n",
    "        \"\"\"\n",
    "        Optimizes the current model pipeline.\n",
    "        \"\"\"\n",
    "\n",
    "        self.pipeline = self._optimize_pipeline(self.pipeline)\n",
    "\n",
    "    \n",
    "    def generate_images(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        num_inference_steps: int = 56,\n",
    "        num_images: int = 2,\n",
    "        save_path: str = \"output\",\n",
    "    ) -> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        Generates images based on the given prompt and saves them to disk.\n",
    "\n",
    "        Parameters:\n",
    "        - prompt: The text prompt to generate images from.\n",
    "        - num_inference_steps: Number of noise removal steps.\n",
    "        - num_images: The number of images to generate. Default is 5.\n",
    "        - save_path: The directory to save the generated images in. Default is \"output\".\n",
    "\n",
    "        Returns:\n",
    "        - images: A list of the generated images.\n",
    "        \"\"\"\n",
    "\n",
    "        images = []\n",
    "        \n",
    "        with torch.xpu.amp.autocast(\n",
    "            enabled=True if self.data_type != torch.float32 else False,\n",
    "            dtype=self.data_type,\n",
    "        ):\n",
    "            outputs = self.pipeline(\n",
    "                prompt=prompt,\n",
    "                num_images_per_prompt=num_images,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=7.0,\n",
    "                #negative_prompt=negative_prompt,\n",
    "            )\n",
    "        for i, image in enumerate(outputs.images):\n",
    "            if not os.path.exists(save_path):\n",
    "                try:\n",
    "                    os.makedirs(save_path)\n",
    "                except OSError as e:\n",
    "                    print(\"Failed to create directory\", save_path, \"due to\", str(e))\n",
    "                    raise\n",
    "            output_image_path = os.path.join(\n",
    "                save_path,\n",
    "                f\"{'_'.join(prompt.split()[:3])}_{i}_{sum(ord(c) for c in prompt) % 10000}.png\",\n",
    "            )\n",
    "            image.save(output_image_path)\n",
    "            images.append(image)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cbdc1b-804b-4bec-8559-e2b4b71c745d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Setting Up the Text-to-Image Interface**\n",
    "\n",
    "The following section is dedicated to creating an intuitive interface right within this notebook, providing a seamless experience to translate your textual prompts into captivating visuals.\n",
    "\n",
    "- **Model Selection**: Pick your desired pre-trained model from the available list.\n",
    "- **Device Selection**: Pick your choice of device.\n",
    "- **Prompt Input**: Enter your creative textual prompt, the muse for the image to be generated.\n",
    "- **Number of Images**: Specify the number of visual interpretations you'd like to see for your prompt.\n",
    "\n",
    "After setting your preferences, all it takes is a button click to witness the text's metamorphosis into imagery!\n",
    "\n",
    "Let's lay the groundwork for this experience:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41a449-ae30-4853-a2b5-d34b6cbdbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mp_img\n",
    "import validators\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import Image as IPImage\n",
    "import ipywidgets as widgets\n",
    "\n",
    "model_cache = {}\n",
    "\n",
    "def prompt_to_image():\n",
    "    out = widgets.Output()\n",
    "    output_dir = \"output\"\n",
    "    model_ids = [\n",
    "        \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    ]\n",
    "    devices = [\n",
    "        \"xpu\",\n",
    "        #\"cpu\"\n",
    "    ]\n",
    "    \n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=model_ids,\n",
    "        value=model_ids[0],\n",
    "        description=\"Model:\",\n",
    "    )\n",
    "    devices_dropdown = widgets.Dropdown(\n",
    "        options=devices,\n",
    "        value=devices[0],\n",
    "        description=\"Device:\",\n",
    "    )\n",
    "    prompt_text = widgets.Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"Enter your prompt\",\n",
    "        description=\"Prompt:\",\n",
    "        layout=widgets.Layout(width=\"600px\")\n",
    "    )    \n",
    "    num_images_slider = widgets.IntSlider(\n",
    "        value=2,\n",
    "        min=1,\n",
    "        max=10,\n",
    "        step=1,\n",
    "        description=\"Images:\",\n",
    "    )    \n",
    "    enhance_checkbox = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description=\"Auto enhance the prompt?\",\n",
    "        disabled=False,\n",
    "        indent=False\n",
    "    )\n",
    "    \n",
    "    layout = widgets.Layout(margin=\"20px\")\n",
    "    button = widgets.Button(description=\"Generate Images!\", button_style=\"primary\")   \n",
    "    model_dropdown.layout.width = \"70%\"\n",
    "    devices_dropdown.layout.width=\"35%\"\n",
    "    enhance_checkbox.layout.width = \"70%\"\n",
    "    enhance_checkbox.layout.margin = \"0 0 0 60px\"\n",
    "    prompt_text.layout.width = \"100%\"\n",
    "    button.layout.margin=\"0 0 0 400px\"\n",
    "    top_row = widgets.HBox([model_dropdown, devices_dropdown, enhance_checkbox])\n",
    "    bottom_row = widgets.HBox([prompt_text])\n",
    "    left_box = widgets.VBox([top_row, bottom_row, num_images_slider])\n",
    "    user_input_widgets = widgets.HBox([left_box], layout=layout)\n",
    "    display(user_input_widgets)\n",
    "    display(button)\n",
    "    display(out)\n",
    "\n",
    "    \n",
    "    def on_submit(button):\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            button.button_style = \"warning\"\n",
    "            print(\"\\nOnce generated, images will be saved to `./output` dir, please wait...\")\n",
    "            selected_model_index = model_ids.index(model_dropdown.value)\n",
    "            selected_device_index = devices.index(devices_dropdown.value)\n",
    "            model_id = model_ids[selected_model_index]\n",
    "            device = devices[selected_device_index]\n",
    "            model_key = (model_id, device)\n",
    "            if model_key not in model_cache:\n",
    "                model_cache[model_key] = Text2ImgModel(model_id, device=device)\n",
    "            prompt = prompt_text.value\n",
    "            num_images = num_images_slider.value\n",
    "            #model = Text2ImgModel(model_id, device=\"xpu\")\n",
    "            model = model_cache[model_key]\n",
    "            \n",
    "            enhancements = [\n",
    "                \"dark\",\n",
    "                \"purple light\",\n",
    "                \"dreaming\",\n",
    "                \"cyberpunk\",\n",
    "                \"ancient\" \", rustic\",\n",
    "                \"gothic\",\n",
    "                \"historical\",\n",
    "                \"punchy\",\n",
    "                \"photo\" \"vivid colors\",\n",
    "                \"4k\",\n",
    "                \"bright\",\n",
    "                \"exquisite\",\n",
    "                \"painting\",\n",
    "                \"art\",\n",
    "                \"fantasy [,/organic]\",\n",
    "                \"detailed\",\n",
    "                \"trending in artstation fantasy\",\n",
    "                \"electric\",\n",
    "                \"night\",\n",
    "                \"whimsical\",\n",
    "                \"surreal\",\n",
    "                \"mystical\",\n",
    "                \"nostalgic\",\n",
    "                \"vibrant\",\n",
    "                \"tranquil\",\n",
    "                \"cinematic\",\n",
    "                \"enchanted\",\n",
    "                \"ethereal\",\n",
    "                \"pastoral\"\n",
    "            ]            \n",
    "            \n",
    "            if not prompt:\n",
    "                prompt = \" \"\n",
    "            if enhance_checkbox.value:\n",
    "                prompt = prompt + \" \" + \" \".join(random.sample(enhancements, 5))\n",
    "                print(f\"Using enhanced prompt: {prompt}\") \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                #output_save_path = \"./\" + prompt.replace(\" \", \"\")\n",
    "                output_save_path =  f\"{'_'.join(prompt.split()[:3])}_{sum(ord(c) for c in prompt) % 10000}\"\n",
    "                model.generate_images(\n",
    "                    prompt,\n",
    "                    num_images=num_images,\n",
    "                    save_path= output_save_path#\"./output\"\n",
    "                )\n",
    "                clear_output(wait=True)\n",
    "                display_generated_images(output_save_path)\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nUser interrupted image generation...\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "            finally:\n",
    "                button.button_style = \"primary\"\n",
    "                #print(\n",
    "                #    f\"Complete generating {num_images} images in './output' in {time.time() - start_time:.2f} seconds.\"\n",
    "                #)\n",
    "                \n",
    "    button.on_click(on_submit)\n",
    "\n",
    "def display_generated_images(output_dir=\"output\"):\n",
    "    image_files = [f for f in os.listdir(output_dir) if f.endswith((\".png\", \".jpg\"))]    \n",
    "    num_images = len(image_files)\n",
    "    num_columns = int(np.ceil(np.sqrt(num_images)))\n",
    "    num_rows = int(np.ceil(num_images / num_columns))\n",
    "    fig, axs = plt.subplots(num_rows, num_columns, figsize=(10 * num_columns / num_columns, 10 * num_rows / num_rows))\n",
    "    if num_images == 1:\n",
    "        axs = np.array([[axs]])\n",
    "    elif num_columns == 1 or num_rows == 1:\n",
    "        axs = np.array([axs])\n",
    "    for ax, image_file in zip(axs.ravel(), image_files):\n",
    "        img = mp_img.imread(os.path.join(output_dir, image_file))\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")  # Hide axes\n",
    "    for ax in axs.ravel()[num_images:]:\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    print(f\"\\nGenerated images...:\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd723f-da20-4cb3-b5e3-fa8653661ebf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Let's Dive In! And Experience the Art of Creation**\n",
    "\n",
    "Set your preferences below, type in your desired visual, and hit the \"Generate Images!\" button to witness the magic of Stable Diffusion.\n",
    "\n",
    "This isn't just about AI; it's about the alchemy of your imagination combined with the prowess of Diffusion models. Every image stands as a symbol of the boundless potential of this synergy.\n",
    "\n",
    "So, why wait? Let's paint the canvas with your imagination!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8d6b65-39b9-4472-b6a6-878e4c2cefdc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run all cells before running this section and wait a few seconds for UI to load\n",
    "prompt_to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c760991-214d-4990-8153-25b5dc032618",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!--#### Reference and Guidelines for Models Used in This Notebook\n",
    "\n",
    "##### CompVis/stable-diffusion-v1-4\n",
    "- **Model card:** [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n",
    "- **License:** CreativeML OpenRAIL M license\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @InProceedings{Rombach_2022_CVPR,\n",
    "        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n",
    "        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n",
    "        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "        month     = {June},\n",
    "        year      = {2022},\n",
    "        pages     = {10684-10695}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "##### stabilityai/stable-diffusion-2\n",
    "- **Model card:** [stabilityai/stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2)\n",
    "- **License:** CreativeML Open RAIL++-M License\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @InProceedings{Rombach_2022_CVPR,\n",
    "        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\n",
    "        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n",
    "        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "        month     = {June},\n",
    "        year      = {2022},\n",
    "        pages     = {10684-10695}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "##### Disclaimer for Using Stable Diffusion Models\n",
    "\n",
    "The stable diffusion models provided here are powerful tools for high-resolution image synthesis, including text-to-image and image-to-image transformations. While they are designed to produce high-quality results, users should be aware of potential limitations:\n",
    "\n",
    "- **Quality Variation:** The quality of generated images may vary based on the complexity of the input text or image, and the alignment with the model's training data.\n",
    "- **Licensing and Usage Constraints:** Please carefully review the licensing information associated with each model to ensure compliance with all terms and conditions.\n",
    "- **Ethical Considerations:** Consider the ethical implications of the generated content, especially in contexts that may involve sensitive or controversial subjects.\n",
    "\n",
    "For detailed information on each model's capabilities, limitations, and best practices, please refer to the respective model cards and associated publications linked below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c16a9-7186-4535-9e77-d67cefbdac35",
   "metadata": {},
   "source": [
    "#### Reference and Guidelines for Models Used in This Notebook\n",
    "\n",
    "##### stabilityai/stable-diffusion-3-medium\n",
    "- **Developed by:** Stability AI\n",
    "- **Model type:** MMDiT text-to-image generative model\n",
    "- **Model Description:** This is a model that can be used to generate images based on text prompts. It is a Multimodal Diffusion Transformer (https://arxiv.org/abs/2403.03206) that uses three fixed, pretrained text encoders (OpenCLIP-ViT/G, CLIP-ViT/L and T5-xxl)\n",
    "\n",
    "#### Disclaimer for Using Stable Diffusion Models\n",
    "\n",
    "The stable diffusion models provided here are powerful tools for high-resolution image synthesis, including text-to-image and image-to-image transformations. While they are designed to produce high-quality results, users should be aware of potential limitations:\n",
    "\n",
    "- **Quality Variation:** The quality of generated images may vary based on the complexity of the input text or image, and the alignment with the model's training data.\n",
    "- **Licensing and Usage Constraints:** Please carefully review the licensing information associated with each model to ensure compliance with all terms and conditions.\n",
    "- **Ethical Considerations:** Consider the ethical implications of the generated content, especially in contexts that may involve sensitive or controversial subjects.\n",
    "\n",
    "For detailed information on each model's capabilities, limitations, and best practices, please refer to the respective model cards and associated publications linked below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xpu",
   "language": "python",
   "name": "xpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
