{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0\n",
    "Copyright (c) 2024, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* Notebook-wide styles */\n",
    ".notebook-container {\n",
    "    font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif;\n",
    "    line-height: 1.6;\n",
    "    color: #333;\n",
    "    background-color: #f5f5f5;\n",
    "}\n",
    "\n",
    "/* Headings */\n",
    "h1, h2, h3, h4, h5, h6 {\n",
    "    font-weight: bold;\n",
    "    margin-top: 40px;\n",
    "    margin-bottom: 20px;\n",
    "    color: #1d1d1f;\n",
    "}\n",
    "\n",
    "/* Images */\n",
    "img {\n",
    "    display: block;\n",
    "    margin: 30px auto;\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1), 0 1px 3px rgba(0, 0, 0, 0.08);\n",
    "    transition: transform 0.3s ease;\n",
    "    border: 2px solid #fff;\n",
    "}\n",
    "\n",
    "img:hover {\n",
    "    transform: translateY(-5px);\n",
    "}\n",
    "\n",
    "/* Image captions */\n",
    ".image-caption {\n",
    "    text-align: center;\n",
    "    font-size: 14px;\n",
    "    color: #666;\n",
    "    margin-top: -10px;\n",
    "    margin-bottom: 30px;\n",
    "}\n",
    "\n",
    "/* Links */\n",
    "a {\n",
    "    color: #007aff;\n",
    "    text-decoration: none;\n",
    "}\n",
    "\n",
    "a:hover {\n",
    "    text-decoration: underline;\n",
    "}\n",
    "\n",
    "/* Code cells */\n",
    ".input_area pre {\n",
    "    background-color: #1d1d1f;\n",
    "    color: #fff;\n",
    "    padding: 15px;\n",
    "    border-radius: 10px;\n",
    "}\n",
    "\n",
    "/* Markdown cells */\n",
    ".text_cell_render {\n",
    "    padding: 30px;\n",
    "    background-color: #fff;\n",
    "    border-radius: 10px;\n",
    "    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1), 0 1px 3px rgba(0, 0, 0, 0.08);\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGe8RF_LzKjK"
   },
   "source": [
    "\n",
    "## A Crash Course in Using LLMs to Build GenAI Powered Applications üöÄ\n",
    "\n",
    "### Introduction üëã\n",
    "Welcome to this crash course on leveraging Large Language Models (LLMs) to infuse intelligence into your applications. In this 30-minute session, we'll explore the key use cases of LLMs and how you can quickly get started with building GenAI powered applications. Get ready to dive in! üåü\n",
    "\n",
    "#### Agenda üìã\n",
    "- Getting Starting with the Intel Developer Cloud ‚òÅÔ∏è\n",
    "- Setting up the environment ‚öôÔ∏è\n",
    "- Zero-shot Chat üéØ\n",
    "    - What is zero-shot learning?\n",
    "    - Why use zero-shot learning?\n",
    "- Few-shot Learning üéì\n",
    "    - What is few-shot learning?\n",
    "    - Why use few-shot learning?\n",
    "- Retrieval Augmented Generation (RAG) üîç\n",
    "    - What is RAG?\n",
    "    - Why use RAG?\n",
    "- Fine-tuning LLMs üöÄ\n",
    "    - What is fine-tuning?\n",
    "    - Why use fine-tuning?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### üåü Getting Started with Intel Developer Cloud üåü\n",
    "To prototype and build your GenAI solution on Intel Developer Cloud, you'll first need to create a free account. Here's how:\n",
    "\n",
    "1. Go to [cloud.intel.com/hoohacks](https://cloud.intel.com/hoohacks) and click \"Get Started\" if you don't have an account yet.\n",
    "   \n",
    "   <img src=\"./images/idc_home_page.png\" alt=\"Intel Developer Cloud homepage\" width=\"500\">\n",
    "   <div class=\"image-caption\"\">Intel Developer Cloud homepage</div>\n",
    "\n",
    "2. Register for a Standard (free) account by filling out the sign-up form.\n",
    "   \n",
    "   <img src=\"./images/idc_service_tiers.png\" alt=\"Intel Developer Cloud service tiers\" width=\"500\">\n",
    "   <div class=\"image-caption\">Intel Developer Cloud service tiers</div>\n",
    "\n",
    "3. Once registered, log in with your new credentials. You'll land on the Intel Developer Cloud Console home page at [https://console.cloud.intel.com](https://console.cloud.intel.com).\n",
    "   \n",
    "   <img src=\"./images/console_genai_essentials.png\" alt=\"Intel Developer Cloud Console - GenAI Essentials\" width=\"500\">\n",
    "   <div class=\"image-caption\">Intel Developer Cloud Console - GenAI Essentials</div>\n",
    "\n",
    "4. From the home page or the left sidebar, navigate to **\"Training\"** and look for the \"Gen AI Essentials\" content. This is where you'll find hands-on tutorials for building GenAI applications.\n",
    "\n",
    "#### ü§ñ Prototyping Fast with Intel Hardware ü§ñ\n",
    "When you launch a Jupyter notebook from GenAI Essentials, it will have shared access to a system with:\n",
    "\n",
    "- **112 CPU cores**\n",
    "- **512 GB RAM**\n",
    "- **1 Intel Data Center GPU Max with 48GB VRAM**\n",
    "\n",
    "This notebook environment is perfect for building your GenAI solution prototype. Go wild and have fun experimenting! üòÑ\n",
    "\n",
    "If you are developing in pytorch, select the `pytorch-gpu` kernel and to use intel GPUs:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "print(f\"GPU is: {torch.xpu.get_device_name()}\")\n",
    "```\n",
    "#### üöÄ Deploying Your Solution üöÄ\n",
    "Once you have a working prototype and are ready to deploy it for others to use, come talk to me. I can provide you with cloud credits to launch a dedicated CPU VM to host your deployed app.\n",
    "\n",
    "To expose the API endpoint, you can use a tool like ngrok to create a public URL.\n",
    "\n",
    "To expose your GenAI application to the internet, you can use ngrok and create a simple Flask server. Here's how:\n",
    "\n",
    "\n",
    "1. Install ngrok and Flask:\n",
    "\n",
    "```python\n",
    "pip install flask ngrok\n",
    "```\n",
    "3. Create a simple Flask server in a new python file (Default port is 5000):\n",
    "```python\n",
    "   from flask import Flask, request, jsonify\n",
    "    app = Flask(__name__)\n",
    "\n",
    "    @app.route('/api', methods=['POST'])\n",
    "    def api():\n",
    "    data = request.get_json()\n",
    "    # Process the request and generate a response\n",
    "    response = {'message': 'Hello, World!'}\n",
    "    return jsonify(response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "```\n",
    "\n",
    "3. In a new python file, set up ngrok to expose the Flask server:\n",
    "\n",
    "```python\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(5000)\n",
    "print(f'Public URL: {public_url}')\n",
    "```\n",
    "4. Run the Flask server and ngrok python files. The public URL printed by ngrok can be used to access your GenAI application from anywhere on the internet.\n",
    "\n",
    "The provided example demonstrates how to create a simple Flask server and expose it using ngrok. You can build upon this foundation to integrate your GenAI application and handle the necessary requests and responses.\n",
    "\n",
    "> **Note:** This is a basic example to get you started. You'll need to modify the Flask server to integrate your GenAI application logic and handle the appropriate requests and responses.\n",
    "\n",
    "This will allow users to access your GenAI application over the internet.\n",
    "\n",
    "Let me know if you have any other questions! I'm excited to see what you build üôå"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lc-U7sQWMFLr"
   },
   "source": [
    "#### Setting up the Environment ‚öôÔ∏è\n",
    "First, let's install the necessary library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pI0jTm47xNj5",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "import sys\n",
    "\n",
    "!echo \"installing required python libraries, please wait...\"\n",
    "!{sys.executable} -m pip install --upgrade predictionguard #> /dev/null # for accessing LLM APIs\n",
    "!{sys.executable} -m pip install --upgrade  \"transformers>=4.38.*\" #> /dev/null\n",
    "!{sys.executable} -m pip install --upgrade  \"datasets>=2.18.*\" #> /dev/null\n",
    "!{sys.executable} -m pip install --upgrade \"accelerate>=0.28.*\" #> /dev/null\n",
    "!{sys.executable} -m pip install --upgrade faiss-cpu #> /dev/null  # for indexing\n",
    "!{sys.executable} -m pip install --upgrade sentence_transformers #> /dev/null # for generating embeddings\n",
    "!echo \"installation complete...\"\n",
    "\n",
    "# add the location where we installed these libraries to the python pkg path (~/.local/lib/python3.9/*)\n",
    "# Get the site-packages directory\n",
    "site_packages_dir = site.getsitepackages()[0]\n",
    "\n",
    "# add the site pkg directory where these pkgs are insalled to the top of sys.path\n",
    "if not os.access(site_packages_dir, os.W_OK):\n",
    "    user_site_packages_dir = site.getusersitepackages()\n",
    "    if user_site_packages_dir in sys.path:\n",
    "        sys.path.remove(user_site_packages_dir)\n",
    "    sys.path.insert(0, user_site_packages_dir)\n",
    "else:\n",
    "    if site_packages_dir in sys.path:\n",
    "        sys.path.remove(site_packages_dir)\n",
    "    sys.path.insert(0, site_packages_dir)\n",
    "\n",
    "\n",
    "# adding ~/.local/bin to PATH as well\n",
    "home_dir = os.path.expanduser('~')\n",
    "bin_path = os.path.join(home_dir, '.local', 'bin')\n",
    "os.environ['PATH'] += os.pathsep + bin_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os4zzcnAMOIt"
   },
   "source": [
    "Next, import the required modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wg7xvnBhxb38",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# FREE access token for usage at: tinyurl.com/pg-intel-hack\n",
    "import predictionguard as pg\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHkRIPHbW3xe"
   },
   "source": [
    "Set up your Prediction Guard access token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_cUA6tClxcM",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "cfe9fcf9-e22d-4fa5-adc6-eb8fede9874b"
   },
   "outputs": [],
   "source": [
    "pg_access_token = getpass('Enter your Prediction Guard access token: ')\n",
    "os.environ['PREDICTIONGUARD_TOKEN'] = pg_access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWtuITjFKRI-"
   },
   "source": [
    "___\n",
    "#### Zero-shot Chat üéØ\n",
    "\n",
    "Zero-shot learning allows LLMs to perform tasks without any explicit training or examples. It leverages the model's pre-existing knowledge to generate responses based on the given prompt.\n",
    "\n",
    "Why use zero-shot learning?\n",
    "\n",
    "- Quick and easy to implement\n",
    "- No need for task-specific training data\n",
    "- Suitable for simple and straightforward tasks\n",
    "\n",
    "Let's try a simple example of zero-shot chat using the Prediction Guard API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qE0JK2xVKQXJ",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "d75af003-9629-4207-f9d0-966f51f52918"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "{\n",
    "\"role\": \"system\",\n",
    "\"content\": \"\"\"You are a Question answer bot and will give an Answer based on the question you get.\n",
    "It is critical to limit your answers to the question and dont print anything else.\n",
    "If you cannot answer the question, respond with 'Sorry, I dont know.'\"\"\"\n",
    "},\n",
    "{\n",
    "\"role\": \"user\",\n",
    "\"content\": \"I am going to meet my friend for a night out on the town.\"\n",
    "}\n",
    "]\n",
    "\n",
    "result = pg.Chat.create(\n",
    "    model=\"Neural-Chat-7B\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(result['choices'][0]['message']['content'].split('\\n')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fr7dHK-VyW2s"
   },
   "source": [
    "___\n",
    "#### Few-shot Learning üéì\n",
    "\n",
    "Few-shot learning involves providing a small number of examples to guide the model's output. It allows LLMs to adapt to specific tasks or styles by learning from a few representative examples.\n",
    "\n",
    "Why use few-shot learning?\n",
    "\n",
    "- Enables task-specific customization\n",
    "- Improves the model's performance on desired tasks\n",
    "- Requires minimal training data\n",
    "\n",
    "Let's explore few-shot learning for linguistic style transfer with the Prediction Guard API:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O-ZbJJBk8fda",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "ec864675-84cf-4566-cf82-eb23a03b04ee"
   },
   "outputs": [],
   "source": [
    "examples = \"\"\"Neutral: \"I'm looking for directions to the nearest bank, can you help me?\"\n",
    "Yoda: \"Directions to the nearest bank, you seek. Help you, I can.\"\n",
    "\n",
    "Neutral: \"It's a pleasure to meet you. What's your name?\"\n",
    "Yoda: \"A pleasure to meet you, it is. Yoda, my name is.\"\n",
    "\n",
    "Neutral: \"I've lost my way, could you point me in the right direction?\"\n",
    "Yoda: \"Lost your way, you have. Point you in the right direction, I will.\"\n",
    "\n",
    "Neutral: \"This weather is wonderful, isn't it?\"\n",
    "Yoda: \"Wonderful, this weather is. Agree, do you not?\"\n",
    "\n",
    "Neutral: \"I'm feeling a bit under the weather today.\"\n",
    "Yoda: \"Under the weather, you are feeling today. Better soon, you will be.\"\n",
    "\n",
    "Neutral: \"Could you please lower the volume? It's quite loud.\"\n",
    "Yoda: \"Lower the volume, could you please? Quite loud, it is.\"\n",
    "\n",
    "Neutral: \"I'm here to collect the documents you mentioned.\"\n",
    "Yoda: \"The documents I mentioned, collect them, you are here to.\"\n",
    "\n",
    "Neutral: \"Thank you for your assistance. I really appreciate it.\"\n",
    "Yoda: \"For your assistance, thank you. Appreciate it, I really do.\"\n",
    "\n",
    "Neutral: \"I'm sorry, I didn't catch your last sentence.\"\n",
    "Yoda: \"Sorry, I am. Your last sentence, catch it, I did not.\"\n",
    "\n",
    "Neutral: \"Let's schedule a meeting for next week to discuss the project.\"\n",
    "Yoda: \"A meeting for next week, schedule, let us. The project, discuss, we will.\"\"\"\n",
    "\n",
    "messages = [\n",
    "{\n",
    "\"role\": \"system\",\n",
    "\"content\": \"You are a text editor that takes in Neutral text from the user and outputs modified text in the way Yoda would speak similar to these examples:\\n\\n\" + examples\n",
    "},\n",
    "{\n",
    "\"role\": \"user\",\n",
    "\"content\": 'Neutral: \"I am going to meet my friend for a night out on the town.\"\\nYoda:'\n",
    "}\n",
    "]\n",
    "\n",
    "for model in [\"Neural-Chat-7B\",\"Hermes-2-Pro-Mistral-7B\", \"Yi-34B-Chat\"]:\n",
    "    result = pg.Chat.create(\n",
    "        model,\n",
    "        messages=messages\n",
    "    )\n",
    "    print(\"=\"*71)\n",
    "    print(f\"Using Model: {model}\")\n",
    "    print(f\"Neutral Text: I am going to meet my friend for a night out on the town.\")\n",
    "    lines = result['choices'][0]['message']['content'].split('\\n')\n",
    "    print(f\"How would Yoda say this?: {lines[0]}\")\n",
    "    print(\"=\"*71)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHTf6uvtZRhz"
   },
   "source": [
    "___\n",
    "#### Retrieval Augmented Generation (RAG) üîç\n",
    "\n",
    "RAG combines LLMs with external knowledge retrieval to generate more informative and accurate responses. It allows models to access and incorporate relevant information from external sources during the generation process.\n",
    "\n",
    "**Why use RAG?**\n",
    "\n",
    "- Enhances the model's knowledge beyond its training data\n",
    "- Generates more accurate and informative responses\n",
    "- Enables the model to handle a wider range of topics and domains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DTaBal3aSGn"
   },
   "source": [
    "##### üöÄ Building a Question Answering RAG pipeline with Sentence Transformers and FAISS üìö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKta2umiamRW"
   },
   "source": [
    "Install required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KiJN2W7F5p8E",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "32229f12-08f7-4991-d24f-af498917a7ea"
   },
   "outputs": [],
   "source": [
    "#!echo \"installing required libraries...\"\n",
    "#!pip install faiss-cpu > /dev/null  # for indexing\n",
    "#!pip install sentence_transformers > /dev/null. # for generating embeddings\n",
    "#!echo \"installing completed...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBi6qsEl8zt_"
   },
   "source": [
    "In this section, we'll explore how to build a powerful question answering system using Sentence Transformers for generating embeddings and FAISS for efficient similarity search. Let's dive in! üåä\n",
    "\n",
    "###### üì• Importing the Required Libraries\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-6pqFJLE5q1",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import predictionguard as pg\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52bTSWro87Co"
   },
   "source": [
    "###### üèóÔ∏è Defining the Knowledge Base\n",
    "Next, we'll define our simplified knowledge base as a list of strings:*italicized text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tx3hXAav87vF",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "knowledge_base = [\n",
    "    \"Prediction Guard is an AI company that provides APIs for language models.\",\n",
    "    \"Prediction Guard is an Intel Liftoff startup\",\n",
    "    \"Intel Liftoff is Intel's premier startup accelerator program for early stage startups\",\n",
    "    \"Prediction Guard offers a variety of models for different tasks like text generation, classification, and question answering.\",\n",
    "    \"Prediction Guard's APIs are easy to use and integrate into your applications.\",\n",
    "    \"Prediction Guard is deployed on the Intel Developer Cloud using Intel Habana Gaudi 2 machines.\",\n",
    "    \"Intel Habana Gaudi 2 is a purpose-built AI processor designed for high-performance deep learning training and inference.\",\n",
    "    \"Gaudi 2 offers high efficiency, scalability, and ease of use for AI workloads.\",\n",
    "    \"By leveraging Gaudi 2 on the Intel Developer Cloud, Prediction Guard can provide powerful and efficient AI capabilities to its users.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_efJE1Zq9J_R"
   },
   "source": [
    "###### üí¨ Creating the Question Answering Prompt Template\n",
    "We'll create a simple prompt template for our question answering system, optionally if you use Langchain you can use the PromptTemplate class to build a composible prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rCmbMD_9LfN",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prompt_template = f\"\"\"\n",
    "### Instruction:\n",
    "Read the below input context and respond with a short answer to the given question.\n",
    "Use only the information in the below input to answer the question.\n",
    "It is critical to limit your answers to the question and dont print anything else.\n",
    "If you cannot answer the question, respond with \"Sorry, I don't know.\"\n",
    "\n",
    "### Input:\n",
    "Context: {{}}\n",
    "Question: {{}}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwYzEgnF9NeX"
   },
   "source": [
    "###### ü§ñ Loading the Sentence Transformer Model\n",
    "We'll load the Sentence Transformer model for generating embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1VWrQnz9Os_"
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\") # a small and fast embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ1Yzoo09SnR"
   },
   "source": [
    "###### üé® Generating Embeddings for the Knowledge Base\n",
    "Using the loaded model, we'll generate embeddings for our knowledge base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQoUkos89TUW"
   },
   "outputs": [],
   "source": [
    "kb_embeddings = model.encode(knowledge_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1J7kZO1g9XYP"
   },
   "source": [
    "###### üîç Initializing the FAISS Index\n",
    "\n",
    "We'll initialize a FAISS index for efficient similarity search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyxNEy589YyT"
   },
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(kb_embeddings.shape[1]) # 384\n",
    "index.add(kb_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcAp-pK39bNx"
   },
   "source": [
    "###### üéØ Defining the Question Answering Function\n",
    "Let's define a function that takes a question, finds the most relevant chunk from the knowledge base, and generates an answer using the language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dxev6F_UAUry",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def rag_answer(question):\n",
    "    try:\n",
    "        # Generate embedding for the question\n",
    "        question_embedding = model.encode([question])\n",
    "\n",
    "        # Find the most similar text from the knowledge base using FAISS\n",
    "        _, most_relevant_idx = index.search(question_embedding, 1)\n",
    "        relevant_chunk = knowledge_base[most_relevant_idx[0][0]]\n",
    "        # Format our prompt with the question and relevant context using f-strings\n",
    "        prompt=prompt_template.format(relevant_chunk, question)\n",
    "\n",
    "        # Get a response from the language model\n",
    "        result = pg.Completion.create(\n",
    "            model=\"Neural-Chat-7B\", #\"Nous-Hermes-Llama2-13B\",\n",
    "            prompt=prompt\n",
    "        )\n",
    "        return result['choices'][0]['text']\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return \"Sorry, something went wrong. Please try again later.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yebqamb9kZ4"
   },
   "source": [
    "üß™ Testing the Question Answering System\n",
    "Let's test our question answering system with a couple of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFcVW45O9nk5",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "ca717512-aa7a-4a92-d978-adfba955481e"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*71)\n",
    "question1 = \"What hardware does Prediction Guard use for its AI services?\"\n",
    "response1 = rag_answer(question1)\n",
    "print(f\"Question 1: {question1}\")\n",
    "print(f\"Response 1: {response1}\")\n",
    "print(\"=\"*71)\n",
    "\n",
    "question2 = \"Where is Prediction Guard's headquarters located?\"\n",
    "response2 = rag_answer(question2)\n",
    "print(f\"Question 2: {question2}\")\n",
    "print(f\"Response 2: {response2}\")\n",
    "print(\"=\"*71)\n",
    "\n",
    "question3 = \"What is Intel Liftoff and what is prediction guards relatioship to liftoff?\"\n",
    "response3 = rag_answer(question3)\n",
    "print(f\"Question 3: {question3}\")\n",
    "print(f\"Response 3: {response3}\")\n",
    "print(\"=\"*71)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulYpJpXn9yuc"
   },
   "source": [
    "As we can see, our question answering system provides a relevant answer for the first question, which can be found in the knowledge base. For the second question, since the information is not present in the knowledge base, it responds with \"Sorry, I don't know.\" üôå"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mElov-HKZiQa"
   },
   "source": [
    "___\n",
    "#### Fine-tuning LLMs üöÄ\n",
    "\n",
    "Fine-tuning involves training LLMs on task-specific data to adapt them to particular domains or applications. It allows for more precise control over the model's behavior and can lead to better performance on specialized tasks.\n",
    "\n",
    "**Why use fine-tuning?**\n",
    "\n",
    "- Achieves the best performance on specific tasks\n",
    "- Enables customization to match desired output style and format\n",
    "- Suitable for complex and domain-specific applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import intel_extension_for_pytorch   # to add intel GPU namespace (torch.xpu) to pytor|ch\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    def get_memory_usage():\n",
    "        memory_reserved = round(torch.xpu.memory_reserved() / 1024**3, 3)\n",
    "        memory_allocated = round(torch.xpu.memory_allocated() / 1024**3, 3)\n",
    "        max_memory_reserved = round(torch.xpu.max_memory_reserved() / 1024**3, 3)\n",
    "        max_memory_allocated = round(torch.xpu.max_memory_allocated() / 1024**3, 3)\n",
    "        return memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated\n",
    "   \n",
    "    def print_memory_usage():\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        print(f\"XPU available!! \\n - Name: {device_name}\")\n",
    "        memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "        memory_usage_text = f\" - XPU Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "        print(f\"\\r{memory_usage_text}\", end=\"\", flush=True)\n",
    "    \n",
    "    print_memory_usage()\n",
    "    torch.xpu.empty_cache()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m58NgKO7gTFR"
   },
   "source": [
    "##### Load the Dataset üìö\n",
    "\n",
    "First, let's subset of the Databricks Dolly 15k dataset using the Hugging Face Datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gp-aq38hgo0F",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlShLHWegsxc"
   },
   "source": [
    "##### Load the Model and Tokenizer ü§ñ\n",
    "Next, we load the `microsoft/phi-1_5` model and tokenizer from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCNut-mXgwSK",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "device = \"cpu\" #\"xpu\" if torch.xpu.is_available() else \"cpu\"\n",
    "model_name = \"microsoft/phi-1_5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device) # move to xpu device if available, otherwise use cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sgL9efhhlUd"
   },
   "source": [
    "##### Perform Inference Before Fine-tuning üîç\n",
    "Let's select a question from the dataset and perform inference before fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfZEf87ag1JP",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question = dataset[42][\"instruction\"]\n",
    "\n",
    "print(\"Inference before fine-tuning:\")\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, truncation=True)\n",
    "output = generator(question, max_length=42)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqAhs1qSg54D"
   },
   "source": [
    "##### Format and Tokenize the Dataset üî†\n",
    "We need to format and tokenize the dataset before fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZzo_nhjhgz5",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def format_dataset(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    sample[\"text\"] = f\"{prompt}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(format_dataset)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Split the dataset into train and test subsets\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üîß Configuring PEFT (Parameter-Efficient Fine-Tuning)\n",
    "\n",
    "We will use parameter efficient fine tuning to tweak the model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=2,\n",
    "    target_modules=[\"fc1\", \"fc2\",\"Wqkv\", \"out_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "#model.gradient_checkpointing_enable()  # enable if low on VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Up Training Arguments üéõÔ∏è\n",
    "Let's set up the training arguments for fine-tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjxhR1MqhBr-"
   },
   "source": [
    "##### Create the Trainer üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
    "Now, we create the trainer object using the model, training arguments, and tokenized dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"output\",\n",
    "        bf16=True,\n",
    "        use_ipex=True,\n",
    "        max_grad_norm=0.6,\n",
    "        weight_decay=0.01,\n",
    "        group_by_length=True,\n",
    "        optim=\"adamw_hf\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=30,\n",
    "        max_steps=200,\n",
    "        #num_train_epochs=3,\n",
    "        report_to=\"wandb\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHzMz7mShZM5",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling \n",
    "from transformers import Trainer\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"  # prevent warnings from training on process forking\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "357sQqvKhEZY"
   },
   "source": [
    "##### Fine-tune the Model üöÄ\n",
    "It's time to fine-tune the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBgqA5yGhWio"
   },
   "outputs": [],
   "source": [
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDWaRCg8hHt_"
   },
   "source": [
    "##### Save the Fine-tuned Model üíæ\n",
    "After fine-tuning, let's save the fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQ0X2tBzhU69"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phf1-EPehK16"
   },
   "source": [
    "##### Perform Inference After Fine-tuning üîç\n",
    "Finally, let's load the fine-tuned model and perform inference again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRswbqw0ekHj",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nInference after fine-tuning with context:\")\n",
    "del fine_tuned_model\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\").to(\"xpu\")\n",
    "prompt = \"What is oneAPI?\"\n",
    "context = \"Intel's OneAPI is a unified programming model that simplifies development across diverse architectures, including CPUs, GPUs, FPGAs, and other accelerators. It provides a set of tools and libraries for high-performance computing, AI, and machine learning workloads.\"\n",
    "input_text = f\"### Instruction\\n{prompt}\\n\\n### Context\\n{context}\\n\\n### Answer\\n\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"xpu\")\n",
    "\n",
    "outputs = fine_tuned_model.generate(input_ids=input_ids, max_length=100, num_return_sequences=1, temperature=0.1, do_sample=True, num_beams=4)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion üéâ\n",
    "Congratulations on completing this crash course on using Large Language Models (LLMs) to build GenAI powered applications! üôå\n",
    "\n",
    "Throughout this session, we've covered a range of techniques and approaches:\n",
    "\n",
    "- We explored zero-shot learning, where you learned how to leverage pre-trained LLMs without additional training data. This powerful technique allows you to tackle a wide range of tasks out of the box. üéØ\n",
    "- Next, we delved into few-shot learning, where you discovered how providing a few examples can significantly improve the performance and accuracy of LLMs on specific tasks. üéì\n",
    "- We then saw Retrieval Augmented Generation (RAG), a technique that combines the power of LLMs with external knowledge retrieval to generate more informed and contextually relevant responses. üîç\n",
    "- Finally, we went through fine-tuning, where you learned how to adapt pre-trained LLMs to specific domains or tasks by training them on a smaller dataset. This enables you to build highly specialized and efficient GenAI applications. üöÄ\n",
    "  \n",
    "With the knowledge and skills you've gained in this crash course, you're now well-equipped to use the potential of LLMs and build intelligent, GenAI-powered applications.\n",
    "\n",
    "\n",
    "Thank you for joining this crash course, and I hope you found it informative and engaging. If you have any further questions or want to dive deeper into any of the topics we covered, feel free to reach out or explore the vast array of resources available online.\n",
    "\n",
    "Happy hacking, and may your GenAI applications be intelligent, intuitive, and impactful! üöÄ‚ú®"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "014fe3d63335407a827b4d982c4ec3dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "149bade753724ad4919c0eff0180045e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b70261992e3e447183ff32aa1478389d",
      "max": 7706,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c8263468b564836b1c3dc071c54d7aa",
      "value": 7706
     }
    },
    "227a392a3b7f4fb0b7304161dd7d3c6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dc50c63ffc74181903da9569acffbda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3074dbf29a2642f7b69bcdeb44da15f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31af2effe3c54a30b467c64390f8945f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_83664fd0fd614104831657b44fa1f250",
       "IPY_MODEL_f1d83c4d98da4a6e8c5de6b92ddd8ccf",
       "IPY_MODEL_32adbbec210f4c4bbcfaa85e0d4f3cf2"
      ],
      "layout": "IPY_MODEL_227a392a3b7f4fb0b7304161dd7d3c6f"
     }
    },
    "32adbbec210f4c4bbcfaa85e0d4f3cf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e7453a475fd4329bece25116e2fa0fa",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_adf6ea880d504bc5b384f64cae721d05",
      "value": "‚Äá7706/7706‚Äá[00:06&lt;00:00,‚Äá1062.23‚Äáexamples/s]"
     }
    },
    "3c8263468b564836b1c3dc071c54d7aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3d433fb9efd348ab9cddee70537e50ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e7453a475fd4329bece25116e2fa0fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83664fd0fd614104831657b44fa1f250": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa1cbff18c974aaf8cebd51abe7a3552",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2dc50c63ffc74181903da9569acffbda",
      "value": "Map:‚Äá100%"
     }
    },
    "a690bb4f6dc94066b95e17c92dfe05cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6e091d2b2214fa6bc96f30b7169fdc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a983719bf8674e73874c1297d8ba911e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eef1f8cbe1c241328572a57b592b2288",
       "IPY_MODEL_149bade753724ad4919c0eff0180045e",
       "IPY_MODEL_eaf28321cdea4d17a9292b166f55ba30"
      ],
      "layout": "IPY_MODEL_db78d44ddd6344a78b3700055c617f6f"
     }
    },
    "aa1cbff18c974aaf8cebd51abe7a3552": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adf6ea880d504bc5b384f64cae721d05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b70261992e3e447183ff32aa1478389d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d247bd3014564420b8aa3343201ec93c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db78d44ddd6344a78b3700055c617f6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eaf28321cdea4d17a9292b166f55ba30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_014fe3d63335407a827b4d982c4ec3dc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d247bd3014564420b8aa3343201ec93c",
      "value": "‚Äá7706/7706‚Äá[00:04&lt;00:00,‚Äá1700.36‚Äáexamples/s]"
     }
    },
    "eef1f8cbe1c241328572a57b592b2288": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a690bb4f6dc94066b95e17c92dfe05cc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3d433fb9efd348ab9cddee70537e50ec",
      "value": "Map:‚Äá100%"
     }
    },
    "f1d83c4d98da4a6e8c5de6b92ddd8ccf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3074dbf29a2642f7b69bcdeb44da15f7",
      "max": 7706,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a6e091d2b2214fa6bc96f30b7169fdc1",
      "value": 7706
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
