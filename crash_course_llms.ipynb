{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a983719bf8674e73874c1297d8ba911e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eef1f8cbe1c241328572a57b592b2288",
              "IPY_MODEL_149bade753724ad4919c0eff0180045e",
              "IPY_MODEL_eaf28321cdea4d17a9292b166f55ba30"
            ],
            "layout": "IPY_MODEL_db78d44ddd6344a78b3700055c617f6f"
          }
        },
        "eef1f8cbe1c241328572a57b592b2288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a690bb4f6dc94066b95e17c92dfe05cc",
            "placeholder": "​",
            "style": "IPY_MODEL_3d433fb9efd348ab9cddee70537e50ec",
            "value": "Map: 100%"
          }
        },
        "149bade753724ad4919c0eff0180045e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b70261992e3e447183ff32aa1478389d",
            "max": 7706,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c8263468b564836b1c3dc071c54d7aa",
            "value": 7706
          }
        },
        "eaf28321cdea4d17a9292b166f55ba30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_014fe3d63335407a827b4d982c4ec3dc",
            "placeholder": "​",
            "style": "IPY_MODEL_d247bd3014564420b8aa3343201ec93c",
            "value": " 7706/7706 [00:04&lt;00:00, 1700.36 examples/s]"
          }
        },
        "db78d44ddd6344a78b3700055c617f6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a690bb4f6dc94066b95e17c92dfe05cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d433fb9efd348ab9cddee70537e50ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b70261992e3e447183ff32aa1478389d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c8263468b564836b1c3dc071c54d7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "014fe3d63335407a827b4d982c4ec3dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d247bd3014564420b8aa3343201ec93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31af2effe3c54a30b467c64390f8945f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83664fd0fd614104831657b44fa1f250",
              "IPY_MODEL_f1d83c4d98da4a6e8c5de6b92ddd8ccf",
              "IPY_MODEL_32adbbec210f4c4bbcfaa85e0d4f3cf2"
            ],
            "layout": "IPY_MODEL_227a392a3b7f4fb0b7304161dd7d3c6f"
          }
        },
        "83664fd0fd614104831657b44fa1f250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa1cbff18c974aaf8cebd51abe7a3552",
            "placeholder": "​",
            "style": "IPY_MODEL_2dc50c63ffc74181903da9569acffbda",
            "value": "Map: 100%"
          }
        },
        "f1d83c4d98da4a6e8c5de6b92ddd8ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3074dbf29a2642f7b69bcdeb44da15f7",
            "max": 7706,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6e091d2b2214fa6bc96f30b7169fdc1",
            "value": 7706
          }
        },
        "32adbbec210f4c4bbcfaa85e0d4f3cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e7453a475fd4329bece25116e2fa0fa",
            "placeholder": "​",
            "style": "IPY_MODEL_adf6ea880d504bc5b384f64cae721d05",
            "value": " 7706/7706 [00:06&lt;00:00, 1062.23 examples/s]"
          }
        },
        "227a392a3b7f4fb0b7304161dd7d3c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa1cbff18c974aaf8cebd51abe7a3552": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dc50c63ffc74181903da9569acffbda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3074dbf29a2642f7b69bcdeb44da15f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e091d2b2214fa6bc96f30b7169fdc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e7453a475fd4329bece25116e2fa0fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adf6ea880d504bc5b384f64cae721d05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## A Crash Course in Using LLMs to Build GenAI Powered Applications 🚀\n",
        "\n",
        "### Introduction 👋\n",
        "Welcome to this crash course on leveraging Large Language Models (LLMs) to infuse intelligence into your applications. In this 30-minute session, we'll explore the key use cases of LLMs and how you can quickly get started with building GenAI powered applications. Get ready to dive in! 🌟\n",
        "\n",
        "#### Agenda 📋\n",
        "- Setting up the environment ⚙️\n",
        "- Zero-shot Chat 🎯\n",
        "    - What is zero-shot learning?\n",
        "    - Why use zero-shot learning?\n",
        "- Few-shot Learning 🎓\n",
        "    - What is few-shot learning?\n",
        "    - Why use few-shot learning?\n",
        "- Retrieval Augmented Generation (RAG) 🔍\n",
        "    - What is RAG?\n",
        "    - Why use RAG?\n",
        "- Fine-tuning LLMs 🚀\n",
        "    - What is fine-tuning?\n",
        "    - Why use fine-tuning?\n",
        "___"
      ],
      "metadata": {
        "id": "ZGe8RF_LzKjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting up the Environment ⚙️\n",
        "First, let's install the necessary library:"
      ],
      "metadata": {
        "id": "lc-U7sQWMFLr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "pI0jTm47xNj5"
      },
      "outputs": [],
      "source": [
        "!pip install predictionguard 2>&1 > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, import the required modules:\n"
      ],
      "metadata": {
        "id": "os4zzcnAMOIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# FREE access token for usage at: tinyurl.com/pg-intel-hack\n",
        "import predictionguard as pg\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "Wg7xvnBhxb38"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up your Prediction Guard access token:"
      ],
      "metadata": {
        "id": "VHkRIPHbW3xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pg_access_token = getpass('Enter your Prediction Guard access token: ')\n",
        "os.environ['PREDICTIONGUARD_TOKEN'] = pg_access_token"
      ],
      "metadata": {
        "id": "K_cUA6tClxcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe9fcf9-e22d-4fa5-adc6-eb8fede9874b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Prediction Guard access token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "#### Zero-shot Chat 🎯\n",
        "\n",
        "Zero-shot learning allows LLMs to perform tasks without any explicit training or examples. It leverages the model's pre-existing knowledge to generate responses based on the given prompt.\n",
        "\n",
        "Why use zero-shot learning?\n",
        "\n",
        "- Quick and easy to implement\n",
        "- No need for task-specific training data\n",
        "- Suitable for simple and straightforward tasks\n",
        "\n",
        "Let's try a simple example of zero-shot chat using the Prediction Guard API:"
      ],
      "metadata": {
        "id": "bWtuITjFKRI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "{\n",
        "\"role\": \"system\",\n",
        "\"content\": \"\"\"You are a Question answer bot and will give an Answer based on the question you get.\n",
        "It is critical to limit your answers to the question and dont print anything else.\n",
        "If you cannot answer the question, respond with 'Sorry, I dont know.'\"\"\"\n",
        "},\n",
        "{\n",
        "\"role\": \"user\",\n",
        "\"content\": \"I am going to meet my friend for a night out on the town.\"\n",
        "}\n",
        "]\n",
        "\n",
        "result = pg.Chat.create(\n",
        "    model=\"Neural-Chat-7B\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(result['choices'][0]['message']['content'].split('\\n')[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE0JK2xVKQXJ",
        "outputId": "d75af003-9629-4207-f9d0-966f51f52918"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enjoy your night out with your friend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "#### Few-shot Learning 🎓\n",
        "\n",
        "Few-shot learning involves providing a small number of examples to guide the model's output. It allows LLMs to adapt to specific tasks or styles by learning from a few representative examples.\n",
        "\n",
        "Why use few-shot learning?\n",
        "\n",
        "- Enables task-specific customization\n",
        "- Improves the model's performance on desired tasks\n",
        "- Requires minimal training data\n",
        "\n",
        "Let's explore few-shot learning for linguistic style transfer with the Prediction Guard API:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fr7dHK-VyW2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = \"\"\"Neutral: \"I'm looking for directions to the nearest bank, can you help me?\"\n",
        "Yoda: \"Directions to the nearest bank, you seek. Help you, I can.\"\n",
        "\n",
        "Neutral: \"It's a pleasure to meet you. What's your name?\"\n",
        "Yoda: \"A pleasure to meet you, it is. Yoda, my name is.\"\n",
        "\n",
        "Neutral: \"I've lost my way, could you point me in the right direction?\"\n",
        "Yoda: \"Lost your way, you have. Point you in the right direction, I will.\"\n",
        "\n",
        "Neutral: \"This weather is wonderful, isn't it?\"\n",
        "Yoda: \"Wonderful, this weather is. Agree, do you not?\"\n",
        "\n",
        "Neutral: \"I'm feeling a bit under the weather today.\"\n",
        "Yoda: \"Under the weather, you are feeling today. Better soon, you will be.\"\n",
        "\n",
        "Neutral: \"Could you please lower the volume? It's quite loud.\"\n",
        "Yoda: \"Lower the volume, could you please? Quite loud, it is.\"\n",
        "\n",
        "Neutral: \"I'm here to collect the documents you mentioned.\"\n",
        "Yoda: \"The documents I mentioned, collect them, you are here to.\"\n",
        "\n",
        "Neutral: \"Thank you for your assistance. I really appreciate it.\"\n",
        "Yoda: \"For your assistance, thank you. Appreciate it, I really do.\"\n",
        "\n",
        "Neutral: \"I'm sorry, I didn't catch your last sentence.\"\n",
        "Yoda: \"Sorry, I am. Your last sentence, catch it, I did not.\"\n",
        "\n",
        "Neutral: \"Let's schedule a meeting for next week to discuss the project.\"\n",
        "Yoda: \"A meeting for next week, schedule, let us. The project, discuss, we will.\"\"\"\n",
        "\n",
        "messages = [\n",
        "{\n",
        "\"role\": \"system\",\n",
        "\"content\": \"You are a text editor that takes in Neutral text from the user and outputs modified text in the way Yoda would speak similar to these examples:\\n\\n\" + examples\n",
        "},\n",
        "{\n",
        "\"role\": \"user\",\n",
        "\"content\": 'Neutral: \"I am going to meet my friend for a night out on the town.\"\\nYoda:'\n",
        "}\n",
        "]\n",
        "\n",
        "for model in [\"Neural-Chat-7B\",\"Hermes-2-Pro-Mistral-7B\", \"Yi-34B-Chat\"]:\n",
        "    result = pg.Chat.create(\n",
        "        model,\n",
        "        messages=messages\n",
        "    )\n",
        "    print(\"=\"*71)\n",
        "    print(f\"Using Model: {model}\")\n",
        "    print(f\"Neutral Text: I am going to meet my friend for a night out on the town.\")\n",
        "    lines = result['choices'][0]['message']['content'].split('\\n')\n",
        "    print(f\"How would Yoda say this?: {lines[0]}\")\n",
        "    print(\"=\"*71)\n"
      ],
      "metadata": {
        "id": "O-ZbJJBk8fda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec864675-84cf-4566-cf82-eb23a03b04ee"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================================\n",
            "Using Model: Neural-Chat-7B\n",
            "Neutral Text: I am going to meet my friend for a night out on the town.\n",
            "How would Yoda say this?: \"A night out on the town, to meet my friend, I am going for.\"\n",
            "=======================================================================\n",
            "=======================================================================\n",
            "Using Model: Hermes-2-Pro-Mistral-7B\n",
            "Neutral Text: I am going to meet my friend for a night out on the town.\n",
            "How would Yoda say this?: \"A night out on the town, you're going. Meet your friend, you will.\" \n",
            "=======================================================================\n",
            "=======================================================================\n",
            "Using Model: Yi-34B-Chat\n",
            "Neutral Text: I am going to meet my friend for a night out on the town.\n",
            "How would Yoda say this?: \"A night out on the town, to meet your friend, you are going. Enjoy, may you.\" \n",
            "=======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "#### Retrieval Augmented Generation (RAG) 🔍\n",
        "\n",
        "RAG combines LLMs with external knowledge retrieval to generate more informative and accurate responses. It allows models to access and incorporate relevant information from external sources during the generation process.\n",
        "\n",
        "**Why use RAG?**\n",
        "\n",
        "- Enhances the model's knowledge beyond its training data\n",
        "- Generates more accurate and informative responses\n",
        "- Enables the model to handle a wider range of topics and domains\n"
      ],
      "metadata": {
        "id": "nHTf6uvtZRhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 🚀 Building a Question Answering RAG pipeline with Sentence Transformers and FAISS 📚"
      ],
      "metadata": {
        "id": "-DTaBal3aSGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required libraries:"
      ],
      "metadata": {
        "id": "LKta2umiamRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"installing required libraries...\"\n",
        "!pip install faiss-cpu > /dev/null  # for indexing\n",
        "!pip install sentence_transformers > /dev/null. # for generating embeddings\n",
        "!echo \"installing completed...\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiJN2W7F5p8E",
        "outputId": "32229f12-08f7-4991-d24f-af498917a7ea"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing required libraries...\n",
            "installing completed...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll explore how to build a powerful question answering system using Sentence Transformers for generating embeddings and FAISS for efficient similarity search. Let's dive in! 🌊\n",
        "\n",
        "###### 📥 Importing the Required Libraries\n",
        "First, let's import the necessary libraries:"
      ],
      "metadata": {
        "id": "dBi6qsEl8zt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import predictionguard as pg\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss"
      ],
      "metadata": {
        "id": "C-6pqFJLE5q1"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 🏗️ Defining the Knowledge Base\n",
        "Next, we'll define our simplified knowledge base as a list of strings:*italicized text*"
      ],
      "metadata": {
        "id": "52bTSWro87Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_base = [\n",
        "    \"Prediction Guard is an AI company that provides APIs for language models.\",\n",
        "    \"Prediction Guard is an Intel Liftoff startup\",\n",
        "    \"Intel Liftoff is Intel's premier startup accelerator program for early stage startups\",\n",
        "    \"Prediction Guard offers a variety of models for different tasks like text generation, classification, and question answering.\",\n",
        "    \"Prediction Guard's APIs are easy to use and integrate into your applications.\",\n",
        "    \"Prediction Guard is deployed on the Intel Developer Cloud using Intel Habana Gaudi 2 machines.\",\n",
        "    \"Intel Habana Gaudi 2 is a purpose-built AI processor designed for high-performance deep learning training and inference.\",\n",
        "    \"Gaudi 2 offers high efficiency, scalability, and ease of use for AI workloads.\",\n",
        "    \"By leveraging Gaudi 2 on the Intel Developer Cloud, Prediction Guard can provide powerful and efficient AI capabilities to its users.\"\n",
        "]"
      ],
      "metadata": {
        "id": "tx3hXAav87vF"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 💬 Creating the Question Answering Prompt Template\n",
        "We'll create a simple prompt template for our question answering system, optionally if you use Langchain you can use the PromptTemplate class to build a composible prompt:"
      ],
      "metadata": {
        "id": "_efJE1Zq9J_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = f\"\"\"\n",
        "### Instruction:\n",
        "Read the below input context and respond with a short answer to the given question.\n",
        "Use only the information in the below input to answer the question.\n",
        "It is critical to limit your answers to the question and dont print anything else.\n",
        "If you cannot answer the question, respond with \"Sorry, I don't know.\"\n",
        "\n",
        "### Input:\n",
        "Context: {{}}\n",
        "Question: {{}}\n",
        "\n",
        "### Response:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_rCmbMD_9LfN"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 🤖 Loading the Sentence Transformer Model\n",
        "We'll load the Sentence Transformer model for generating embeddings:"
      ],
      "metadata": {
        "id": "zwYzEgnF9NeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\") # a small and fast embedding model"
      ],
      "metadata": {
        "id": "B1VWrQnz9Os_"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 🎨 Generating Embeddings for the Knowledge Base\n",
        "Using the loaded model, we'll generate embeddings for our knowledge base:"
      ],
      "metadata": {
        "id": "lJ1Yzoo09SnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kb_embeddings = model.encode(knowledge_base)"
      ],
      "metadata": {
        "id": "lQoUkos89TUW"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 🔍 Initializing the FAISS Index\n",
        "\n",
        "We'll initialize a FAISS index for efficient similarity search:"
      ],
      "metadata": {
        "id": "1J7kZO1g9XYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatL2(kb_embeddings.shape[1]) # 384\n",
        "index.add(kb_embeddings)"
      ],
      "metadata": {
        "id": "uyxNEy589YyT"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 🎯 Defining the Question Answering Function\n",
        "Let's define a function that takes a question, finds the most relevant chunk from the knowledge base, and generates an answer using the language model:"
      ],
      "metadata": {
        "id": "WcAp-pK39bNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_answer(question):\n",
        "    try:\n",
        "        # Generate embedding for the question\n",
        "        question_embedding = model.encode([question])\n",
        "\n",
        "        # Find the most similar text from the knowledge base using FAISS\n",
        "        _, most_relevant_idx = index.search(question_embedding, 1)\n",
        "        relevant_chunk = knowledge_base[most_relevant_idx[0][0]]\n",
        "        # Format our prompt with the question and relevant context using f-strings\n",
        "        prompt=prompt_template.format(relevant_chunk, question)\n",
        "\n",
        "        # Get a response from the language model\n",
        "        result = pg.Completion.create(\n",
        "            model=\"Neural-Chat-7B\", #\"Nous-Hermes-Llama2-13B\",\n",
        "            prompt=prompt\n",
        "        )\n",
        "        return result['choices'][0]['text']\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        return \"Sorry, something went wrong. Please try again later.\""
      ],
      "metadata": {
        "id": "Dxev6F_UAUry"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧪 Testing the Question Answering System\n",
        "Let's test our question answering system with a couple of examples:"
      ],
      "metadata": {
        "id": "8yebqamb9kZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*71)\n",
        "question1 = \"What hardware does Prediction Guard use for its AI services?\"\n",
        "response1 = rag_answer(question1)\n",
        "print(f\"Question 1: {question1}\")\n",
        "print(f\"Response 1: {response1}\")\n",
        "print(\"=\"*71)\n",
        "\n",
        "question2 = \"Where is Prediction Guard's headquarters located?\"\n",
        "response2 = rag_answer(question2)\n",
        "print(f\"Question 2: {question2}\")\n",
        "print(f\"Response 2: {response2}\")\n",
        "print(\"=\"*71)\n",
        "\n",
        "question3 = \"What is Intel Liftoff and what is prediction guards relatioship to liftoff?\"\n",
        "response3 = rag_answer(question3)\n",
        "print(f\"Question 3: {question3}\")\n",
        "print(f\"Response 3: {response3}\")\n",
        "print(\"=\"*71)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFcVW45O9nk5",
        "outputId": "ca717512-aa7a-4a92-d978-adfba955481e"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================================\n",
            "Question 1: What hardware does Prediction Guard use for its AI services?\n",
            "Response 1: Prediction Guard uses Intel Habana Gaudi 2 machines for its AI services.\n",
            "=======================================================================\n",
            "Question 2: Where is Prediction Guard's headquarters located?\n",
            "Response 2: Sorry, I don't know.\n",
            "=======================================================================\n",
            "Question 1: What is Intel Liftoff and what is prediction guards relatioship to liftoff?\n",
            "Response 1: Intel Liftoff is a startup incubator that supports promising technology companies. Prediction Guard is one such startup that is a part of the Intel Liftoff program. The relationship between Prediction Guard and Intel Liftoff is that Prediction Guard is an Intel Liftoff startup which is being nurtured and supported by the incubator program.\n",
            "=======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, our question answering system provides a relevant answer for the first question, which can be found in the knowledge base. For the second question, since the information is not present in the knowledge base, it responds with \"Sorry, I don't know.\" 🙌"
      ],
      "metadata": {
        "id": "ulYpJpXn9yuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "#### Fine-tuning LLMs 🚀\n",
        "\n",
        "Fine-tuning involves training LLMs on task-specific data to adapt them to particular domains or applications. It allows for more precise control over the model's behavior and can lead to better performance on specialized tasks.\n",
        "\n",
        "**Why use fine-tuning?**\n",
        "\n",
        "- Achieves the best performance on specific tasks\n",
        "- Enables customization to match desired output style and format\n",
        "- Suitable for complex and domain-specific applications"
      ],
      "metadata": {
        "id": "mElov-HKZiQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --no-cache datasets transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J3Qla6sXfO1Q",
        "outputId": "f23a7d50-af11-40a6-ad8a-0884832b8846"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.36.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.39.0-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec[http]<=2024.2.0,>=2023.1.0 (from datasets)\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m142.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: fsspec, transformers, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.3.1\n",
            "    Uninstalling fsspec-2024.3.1:\n",
            "      Successfully uninstalled fsspec-2024.3.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.36.2\n",
            "    Uninstalling transformers-4.36.2:\n",
            "      Successfully uninstalled transformers-4.36.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.16.1\n",
            "    Uninstalling datasets-2.16.1:\n",
            "      Successfully uninstalled datasets-2.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.25.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.1 which is incompatible.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.18.0 fsspec-2024.2.0 transformers-4.39.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "datasets",
                  "fsspec",
                  "transformers"
                ]
              },
              "id": "623afe897d7e4125bda559a0f0c273f0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Load the Dataset 📚\n",
        "\n",
        "First, let's subset of the Dolly 15k dataset using the Hugging Face Datasets library that only contains question & answers:"
      ],
      "metadata": {
        "id": "m58NgKO7gTFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"dave-does-data/databricks-dolly-qa-subset-7k\", split=\"train\")"
      ],
      "metadata": {
        "id": "gp-aq38hgo0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Load the Model and Tokenizer 🤖\n",
        "Next, we load the distilgpt2 model and tokenizer from Hugging Face:"
      ],
      "metadata": {
        "id": "qlShLHWegsxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "BCNut-mXgwSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Perform Inference Before Fine-tuning 🔍\n",
        "Let's select a question from the dataset and perform inference before fine-tuning:"
      ],
      "metadata": {
        "id": "0sgL9efhhlUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question = dataset[42][\"instruction\"]\n",
        "\n",
        "print(\"Inference before fine-tuning:\")\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "output = generator(question, max_length=100)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "FfZEf87ag1JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Tokenize the Dataset 🔠\n",
        "We need to tokenize the dataset before fine-tuning:"
      ],
      "metadata": {
        "id": "EqAhs1qSg54D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"instruction\"], examples[\"response\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)"
      ],
      "metadata": {
        "id": "oZzo_nhjhgz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Set Up Training Arguments 🎛️\n",
        "Let's set up the training arguments for fine-tuning:"
      ],
      "metadata": {
        "id": "b_q6sL4ag964"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    max_steps=50,\n",
        "    bf16=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "id": "7MnHkZJ_hdoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create the Trainer 🏋️‍♀️\n",
        "Now, we create the trainer object using the model, training arguments, and tokenized dataset:"
      ],
      "metadata": {
        "id": "tjxhR1MqhBr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "KHzMz7mShZM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fine-tune the Model 🚀\n",
        "It's time to fine-tune the model:"
      ],
      "metadata": {
        "id": "357sQqvKhEZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "FBgqA5yGhWio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save the Fine-tuned Model 💾\n",
        "After fine-tuning, let's save the fine-tuned model:"
      ],
      "metadata": {
        "id": "PDWaRCg8hHt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./fine_tuned_model\")"
      ],
      "metadata": {
        "id": "mQ0X2tBzhU69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Perform Inference After Fine-tuning 🔍\n",
        "Finally, let's load the fine-tuned model and perform inference again:"
      ],
      "metadata": {
        "id": "phf1-EPehK16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
        "\n",
        "print(\"\\nInference after fine-tuning:\")\n",
        "generator = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n",
        "output = generator(question, max_length=100)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "zlvBZd8chSVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the Dolly 15k dataset\n",
        "dataset = load_dataset(\"dave-does-data/databricks-dolly-qa-subset-7k\", split=\"train\")\n",
        "\n",
        "# Load the distilgpt2 model and tokenizer from Hugging Face\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Select a question from the Dolly 15k dataset\n",
        "question = dataset[42][\"instruction\"]\n",
        "\n",
        "# Perform inference before fine-tuning\n",
        "print(\"Inference before fine-tuning:\")\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "output = generator(question, max_length=100)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", output[0][\"generated_text\"])\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    # Adding a prompt to each question and answer pair\n",
        "    prompt_examples = [\"Question: \" + instr + \" Answer: \" + resp for instr, resp in zip(examples['instruction'], examples['response'])]\n",
        "    # Tokenize the prompt examples\n",
        "    tokenized_examples = tokenizer(prompt_examples, truncation=True, padding=\"max_length\", max_length=128)\n",
        "    # The labels should be the same as input_ids for causal language modeling\n",
        "    tokenized_examples[\"labels\"] = tokenized_examples[\"input_ids\"].copy()\n",
        "    return tokenized_examples\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "# Set up the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    max_steps=50,\n",
        "    bf16=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Create the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./fine_tuned_model\")\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
        "\n",
        "# Perform inference after fine-tuning\n",
        "print(\"\\nInference after fine-tuning:\")\n",
        "generator = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)\n",
        "output = generator(question, max_length=100)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322,
          "referenced_widgets": [
            "a983719bf8674e73874c1297d8ba911e",
            "eef1f8cbe1c241328572a57b592b2288",
            "149bade753724ad4919c0eff0180045e",
            "eaf28321cdea4d17a9292b166f55ba30",
            "db78d44ddd6344a78b3700055c617f6f",
            "a690bb4f6dc94066b95e17c92dfe05cc",
            "3d433fb9efd348ab9cddee70537e50ec",
            "b70261992e3e447183ff32aa1478389d",
            "3c8263468b564836b1c3dc071c54d7aa",
            "014fe3d63335407a827b4d982c4ec3dc",
            "d247bd3014564420b8aa3343201ec93c",
            "31af2effe3c54a30b467c64390f8945f",
            "83664fd0fd614104831657b44fa1f250",
            "f1d83c4d98da4a6e8c5de6b92ddd8ccf",
            "32adbbec210f4c4bbcfaa85e0d4f3cf2",
            "227a392a3b7f4fb0b7304161dd7d3c6f",
            "aa1cbff18c974aaf8cebd51abe7a3552",
            "2dc50c63ffc74181903da9569acffbda",
            "3074dbf29a2642f7b69bcdeb44da15f7",
            "a6e091d2b2214fa6bc96f30b7169fdc1",
            "4e7453a475fd4329bece25116e2fa0fa",
            "adf6ea880d504bc5b384f64cae721d05"
          ]
        },
        "id": "bwgqNFWtZpXu",
        "outputId": "8d33c655-8261-4c1e-eaea-4a797d347859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference before fine-tuning:\n",
            "Question: What happens when someone throws a phone onto a mattress?\n",
            "Answer: What happens when someone throws a phone onto a mattress? What happens when people are sleeping? What happens when your phone hits the floor?\n",
            "\n",
            "\n",
            "It looks like the real estate industry can't even get past that. As The Post reports, \"Most Americans are in a better position than in the past.\"\n",
            "The company also says it is aware this is not a big deal.\n",
            "But we've learned from the Post and can still be confident it will follow in the coming months.\n",
            "What\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7706 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a983719bf8674e73874c1297d8ba911e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7706 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31af2effe3c54a30b467c64390f8945f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bRswbqw0ekHj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}