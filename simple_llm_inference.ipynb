{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8b40b326-4549-4b2b-8ce1-a453fbaa7a19",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0\n",
    "Copyright (c) 2023, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d173d6a-a86d-441a-b36a-efed57814310",
   "metadata": {},
   "source": [
    "---\n",
    "**Simple LLM Inference: Playing with Language Models on Intel® Data Center Max Series GPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f2e84-54e8-4c1d-bfec-302f9dff577d",
   "metadata": {},
   "source": [
    "Hello and welcome! Are you curious about how computers understand and generate human-like text? Do you want to play around with text generation without getting too technical? Then you've come to the right place.\n",
    "\n",
    "Large Language Models (LLMs) have a wide range of applications, but they can also be fun to experiment with. Here, we'll use some simple pre-trained models to explore text generation interactively.\n",
    "\n",
    "Powered by Intel® Data Center GPU Max 1100s, this notebook provides a hands-on experience that doesn't require deep technical knowledge. Whether you're a student, writer, educator, or just curious about AI, this guide is designed for you.\n",
    "\n",
    "Ready to try it out? Let's set up our environment and start exploring the world of text generation with LLMs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e47145a5-c2b2-4957-8ce0-51c0fb1bf9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation in progress...\n",
      "WARNING conda.models.version:get_matcher(548): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\n",
      "Installation successful\n"
     ]
    }
   ],
   "source": [
    "# install and import required dependencies\n",
    "!echo \"Installation in progress...\"\n",
    "!conda install -y --quiet -c conda-forge \\\n",
    "    accelerate==0.23.0 \\\n",
    "    validators==0.22.0 \\\n",
    "    transformers==4.32.1 \\\n",
    "    sentencepiece \\\n",
    "    pillow \\\n",
    "    ipywidgets \\\n",
    "    ipython > /dev/null && echo \"Installation successful\" || echo \"Installation failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abc93ee-117e-4d10-a97c-c26429b38159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "from ipywidgets import VBox, HBox, Button, Dropdown, IntSlider, FloatSlider, Text, Output, Label, Layout\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# random seed\n",
    "if torch.xpu.is_available():\n",
    "    seed = 88\n",
    "    random.seed(seed)\n",
    "    torch.xpu.manual_seed(seed)\n",
    "    torch.xpu.manual_seed_all(seed)\n",
    "\n",
    "def select_device(preferred_device=None):\n",
    "    \"\"\"\n",
    "    Selects the best available XPU device or the preferred device if specified.\n",
    "\n",
    "    Args:\n",
    "        preferred_device (str, optional): Preferred device string (e.g., \"cpu\", \"xpu\", \"xpu:0\", \"xpu:1\", etc.). If None, a random available XPU device will be selected or CPU if no XPU devices are available.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The selected device object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if preferred_device and preferred_device.startswith(\"cpu\"):\n",
    "            print(\"Using CPU.\")\n",
    "            return torch.device(\"cpu\")\n",
    "        if preferred_device and preferred_device.startswith(\"xpu\"):\n",
    "            if preferred_device == \"xpu\" or (\n",
    "                \":\" in preferred_device\n",
    "                and int(preferred_device.split(\":\")[1]) >= torch.xpu.device_count()\n",
    "            ):\n",
    "                preferred_device = (\n",
    "                    None  # Handle as if no preferred device was specified\n",
    "                )\n",
    "            else:\n",
    "                device = torch.device(preferred_device)\n",
    "                if device.type == \"xpu\" and device.index < torch.xpu.device_count():\n",
    "                    vram_used = torch.xpu.memory_allocated(device) / (\n",
    "                        1024**2\n",
    "                    )  # In MB\n",
    "                    print(\n",
    "                        f\"Using preferred device: {device}, VRAM used: {vram_used:.2f} MB\"\n",
    "                    )\n",
    "                    return device\n",
    "\n",
    "        if torch.xpu.is_available():\n",
    "            device_id = random.choice(\n",
    "                range(torch.xpu.device_count())\n",
    "            )  # Select a random available XPU device\n",
    "            device = torch.device(f\"xpu:{device_id}\")\n",
    "            vram_used = torch.xpu.memory_allocated(device) / (1024**2)  # In MB\n",
    "            print(f\"Selected device: {device}, VRAM used: {vram_used:.2f} MB\")\n",
    "            return device\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while selecting the device: {e}\")\n",
    "    print(\"No XPU devices available or preferred device not found. Using CPU.\")\n",
    "    return torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416381a3-6293-4701-a8fa-76f1402037dc",
   "metadata": {},
   "source": [
    "---\n",
    "**A Glimpse Into Text Generation with Language Models**\n",
    "\n",
    "If you're intrigued by how machines can generate human-like text, let's take a closer look at the underlying code. Even if you're not technically inclined, this section will provide a high-level understanding of how it all works.:\n",
    "\n",
    "- **Class Definition**: The `ChatBotModel` class is the core of our text generation. It handles the setup, optimization, and interaction with the LLM (Large Language Model).\n",
    "\n",
    "- **Initialization**: When you create an instance of this class, you can specify the model's path, the device to run on (defaulting to Intel's \"xpu\" device if available), and the data type. There's also an option to optimize the model for Intel GPUs using Intel Extension For PyTorch (IPEX).\n",
    "\n",
    "- **Input Preparation**: The `prepare_input` method ensures that the input doesn't exceed the maximum length and combines the previous text with the user input, if required.\n",
    "\n",
    "- **Output Generation**: The `gen_output` method takes the prepared input and several parameters controlling the generation process, like temperature, top_p, top_k, etc., and produces the text response.\n",
    "\n",
    "- **Warm-up**: Before the main interactions, the `warmup_model` method helps in \"warming up\" the model to make subsequent runs faster.\n",
    "\n",
    "- **Text Processing**: Several methods like `unique_sentences`, `remove_repetitions`, and `extract_bot_response` handle the text processing to ensure the generated text is readable and free from repetitions or unnecessary echoes.\n",
    "\n",
    "Feel free to explore the code and play around with different parameters. Remember, this is a simple and interactive way to experiment with text generation. It's not a cutting-edge chatbot, but rather a playful tool to engage with language models. Enjoy the journey into the world of LLMs, using Intel® Data Center GPU Max 1100s!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2f29e9-cd41-4605-aafc-f5aaaf440469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBotModel:\n",
    "    \"\"\"\n",
    "    ChatBotModel is a class for generating responses based on text prompts using a pretrained model.\n",
    "\n",
    "    Attributes:\n",
    "    - device: The device to run the model on. Default is \"xpu\" if available, otherwise \"cpu\".\n",
    "    - model: The loaded model for text generation.\n",
    "    - tokenizer: The loaded tokenizer for the model.\n",
    "    - torch_dtype: The data type to use in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path: str = \"openlm-research/open_llama_3b_v2\",  # \"Writer/camel-5b-hf\",\n",
    "        torch_dtype: torch.dtype = torch.bfloat16,\n",
    "        optimize: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        The initializer for ChatBotModel class.\n",
    "\n",
    "        Parameters:\n",
    "        - model_id_or_path: The identifier or path of the pretrained model.\n",
    "        - torch_dtype: The data type to use in the model. Default is torch.bfloat16.\n",
    "        - optimize: If True, ipex is used to optimized the model\n",
    "        \"\"\"'\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.device = select_device(\"xpu\")\n",
    "        if (\n",
    "            self.device == self.device.startswith(\"xpu\")\n",
    "            if isinstance(self.device, str)\n",
    "            else self.device.type == \"xpu\"\n",
    "        ):\n",
    "\n",
    "            self.autocast = torch.xpu.amp.autocast\n",
    "        else:\n",
    "            self.autocast = torch.cpu.amp.autocast\n",
    "        self.torch_dtype = torch_dtype\n",
    "\n",
    "        if \"llama\" in model_id_or_path:\n",
    "            self.tokenizer = LlamaTokenizer.from_pretrained(model_id_or_path)\n",
    "            self.model = (\n",
    "                LlamaForCausalLM.from_pretrained(\n",
    "                    model_id_or_path,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    torch_dtype=self.torch_dtype,\n",
    "                    cache_dir=\"./llm/models\",\n",
    "                )\n",
    "                .to(self.device)\n",
    "                .eval()\n",
    "            )\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_id_or_path, trust_remote_code=True, cache_dir=\"./llm/models\"\n",
    "            )\n",
    "            self.model = (\n",
    "                AutoModelForCausalLM.from_pretrained(\n",
    "                    model_id_or_path,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=self.torch_dtype,\n",
    "                    cache_dir=\"./llm/models\",\n",
    "                )\n",
    "                .to(self.device)\n",
    "                .eval()\n",
    "            )\n",
    "        self.max_length = 256\n",
    "        print(f\"Using max length: {self.max_length}\")\n",
    "\n",
    "        if optimize:\n",
    "            if hasattr(ipex, \"optimize_transformers\"):\n",
    "                try:\n",
    "                    ipex.optimize_transformers(self.model, dtype=self.torch_dtype)\n",
    "                except:\n",
    "                    ipex.optimize(self.model, dtype=self.torch_dtype)\n",
    "            else:\n",
    "                ipex.optimize(self.model, dtype=self.torch_dtype)\n",
    "\n",
    "    def prepare_input(self, previous_text, user_input):\n",
    "        \"\"\"Prepare the input for the model, ensuring it doesn't exceed the maximum length.\"\"\"\n",
    "        response_buffer = 100\n",
    "        combined_text = previous_text + \"\\nUser: \" + user_input + \"\\nBot: \"\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            combined_text, return_tensors=\"pt\", truncation=False\n",
    "        )\n",
    "        adjusted_max_length = self.max_length - response_buffer\n",
    "        if input_ids.shape[1] > adjusted_max_length:\n",
    "            input_ids = input_ids[:, -adjusted_max_length:]\n",
    "        return input_ids.to(device=self.device)\n",
    "\n",
    "    def gen_output(\n",
    "        self, input_ids, temperature, top_p, top_k, num_beams, repetition_penalty\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate the output text based on the given input IDs and generation parameters.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): The input tensor containing token IDs.\n",
    "            temperature (float): The temperature for controlling randomness in Boltzmann distribution.\n",
    "                                Higher values increase randomness, lower values make the generation more deterministic.\n",
    "            top_p (float): The cumulative distribution function (CDF) threshold for Nucleus Sampling.\n",
    "                           Helps in controlling the trade-off between randomness and diversity.\n",
    "            top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "            num_beams (int): The number of beams for beam search. Controls the breadth of the search.\n",
    "            repetition_penalty (float): The penalty applied for repeating tokens.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The generated output tensor.\n",
    "        \"\"\"\n",
    "        with self.autocast(\n",
    "            enabled=True if self.torch_dtype != torch.float32 else False,\n",
    "            dtype=self.torch_dtype,\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                output = self.model.generate(\n",
    "                    input_ids,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    max_length=self.max_length,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    num_beams=num_beams,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                )\n",
    "                return output\n",
    "\n",
    "    def warmup_model(\n",
    "        self, temperature, top_p, top_k, num_beams, repetition_penalty\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Warms up the model by generating a sample response.\n",
    "        \"\"\"\n",
    "        sample_prompt = \"\"\"A dialog, where User interacts with a helpful Bot.\n",
    "        AI is helpful, kind, obedient, honest, and knows its own limits.\n",
    "        User: Hello, Bot.\n",
    "        Bot: Hello! How can I assist you today?\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer(sample_prompt, return_tensors=\"pt\").input_ids.to(\n",
    "            device=self.device\n",
    "        )\n",
    "        _ = self.gen_output(\n",
    "            input_ids,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "        )\n",
    "\n",
    "    def unique_sentences(self, text: str) -> str:\n",
    "        sentences = text.split(\". \")\n",
    "        if sentences[-1] and sentences[-1][-1] != \".\":\n",
    "            sentences = sentences[:-1]\n",
    "        sentences = set(sentences)\n",
    "        return \". \".join(sentences) + \".\" if sentences else \"\"\n",
    "\n",
    "    def remove_repetitions(self, text: str, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove repetitive sentences or phrases from the generated text and avoid echoing user's input.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text with potential repetitions.\n",
    "            user_input (str): The user's original input to check against echoing.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed text with repetitions and echoes removed.\n",
    "        \"\"\"\n",
    "        text = re.sub(re.escape(user_input), \"\", text, count=1).strip()\n",
    "        text = self.unique_sentences(text)\n",
    "        return text\n",
    "\n",
    "    def extract_bot_response(self, generated_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the first response starting with \"Bot:\" from the generated text.\n",
    "\n",
    "        Args:\n",
    "            generated_text (str): The full generated text from the model.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted response starting with \"Bot:\".\n",
    "        \"\"\"\n",
    "        prefix = \"Bot:\"\n",
    "        generated_text = generated_text.replace(\"\\n\", \". \")\n",
    "        bot_response_start = generated_text.find(prefix)\n",
    "        if bot_response_start != -1:\n",
    "            response_start = bot_response_start + len(prefix)\n",
    "            end_of_response = generated_text.find(\"\\n\", response_start)\n",
    "            if end_of_response != -1:\n",
    "                return generated_text[response_start:end_of_response].strip()\n",
    "            else:\n",
    "                return generated_text[response_start:].strip()\n",
    "        return re.sub(r'^[^a-zA-Z0-9]+', '', generated_text)\n",
    "\n",
    "    def interact(\n",
    "        self,\n",
    "        out: Output,  # Output widget to display the conversation\n",
    "        with_context: bool = True,\n",
    "        temperature: float = 0.10,\n",
    "        top_p: float = 0.95,\n",
    "        top_k: int = 40,\n",
    "        num_beams: int = 3,\n",
    "        repetition_penalty: float = 1.80,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Handle the chat loop where the user provides input and receives a model-generated response.\n",
    "\n",
    "        Args:\n",
    "            with_context (bool): Whether to consider previous interactions in the session. Default is True.\n",
    "            temperature (float): The temperature for controlling randomness in Boltzmann distribution.\n",
    "                                 Higher values increase randomness, lower values make the generation more deterministic.\n",
    "            top_p (float): The cumulative distribution function (CDF) threshold for Nucleus Sampling.\n",
    "                           Helps in controlling the trade-off between randomness and diversity.\n",
    "            top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "            num_beams (int): The number of beams for beam search. Controls the breadth of the search.\n",
    "            repetition_penalty (float): The penalty applied for repeating tokens.\n",
    "            \"\"\"\n",
    "        previous_text = \"\"\n",
    "    \n",
    "        def display_user_input_widgets():\n",
    "            user_input_widget = Text(placeholder=\"Type your message here...\", layout=Layout(width='80%'))\n",
    "            send_button = Button(description=\"Send\", layout=Layout(width='10%'))\n",
    "            display(HBox([user_input_widget, send_button]))\n",
    "            def on_send(button):\n",
    "                nonlocal previous_text\n",
    "                user_input = user_input_widget.value\n",
    "                if user_input.lower() == \"exit\":\n",
    "                    return\n",
    "                if with_context:\n",
    "                    input_ids = self.prepare_input(previous_text, user_input)\n",
    "                else:\n",
    "                    input_ids = self.tokenizer.encode(user_input, return_tensors=\"pt\").to(self.device)\n",
    "    \n",
    "                output_ids = self.gen_output(\n",
    "                    input_ids,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    num_beams=num_beams,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                )\n",
    "                generated_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                generated_text = self.extract_bot_response(generated_text)\n",
    "                generated_text = self.remove_repetitions(generated_text, user_input)\n",
    "    \n",
    "                with out:\n",
    "                    print(f\"You: {user_input}\")\n",
    "                    print(f\"Bot: {generated_text}\")\n",
    "    \n",
    "                if with_context:\n",
    "                    previous_text += \"\\nUser: \" + user_input + \"\\nBot: \" + generated_text\n",
    "                user_input_widget.value = \"\" \n",
    "                display_user_input_widgets()\n",
    "            send_button.on_click(on_send)\n",
    "        display_user_input_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a61ad2-5155-4b07-9f10-d9d18f252e8f",
   "metadata": {},
   "source": [
    "---\n",
    "**Setting Up the Interactive Text Generation Interface**\n",
    "\n",
    "In the next section, we'll create an interactive text generation interface right here in this notebook. This will enable you to select a model, provide a prompt, and tweak various parameters without touching the code itself.\n",
    "\n",
    "- **Model Selection**: Choose from available pre-trained models or enter a custom model from the HuggingFace Hub.\n",
    "- **Interaction Mode**: Decide whether to interact with or without context, allowing the model to remember previous interactions or treat each input independently.\n",
    "- **Temperature**: Adjust this parameter to control the randomness in text generation. Higher values increase creativity; lower values make the generation more deterministic.\n",
    "- **Top_p, Top_k**: Play with these parameters to influence the diversity and quality of the generated text.\n",
    "- **Number of Beams**: Control the breadth of the search in text generation.\n",
    "- **Repetition Penalty**: Modify this to prevent or allow repeated phrases and sentences.\n",
    "\n",
    "Once you've set your preferences, you can start the interaction and even reset or reload the model to try different settings. Let's set this up and explore the playful world of text generation using Intel® Data Center GPU Max 1100s!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42264502-ffdd-47e1-af1e-f7407c4b365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    models = [\"openlm-research/open_llama_3b_v2\", \"Writer/camel-5b-hf\", ]\n",
    "    interaction_modes = [\"Interact with context\", \"Interact without context\"]\n",
    "    model_dropdown = Dropdown(options=models, value=models[0], description=\"Model:\")\n",
    "    interaction_mode = Dropdown(options=interaction_modes, value=interaction_modes[1], description=\"Interaction:\")\n",
    "    temperature_slider = FloatSlider(value=0.10, min=0, max=1, step=0.01, description=\"Temperature:\")\n",
    "    top_p_slider = FloatSlider(value=0.95, min=0, max=1, step=0.01, description=\"Top P:\")\n",
    "    top_k_slider = IntSlider(value=40, min=0, max=100, step=1, description=\"Top K:\")\n",
    "    num_beams_slider = IntSlider(value=3, min=1, max=10, step=1, description=\"Num Beams:\")\n",
    "    repetition_penalty_slider = FloatSlider(value=1.80, min=0, max=2, step=0.1, description=\"Rep Penalty:\")\n",
    "    \n",
    "    out = Output()    \n",
    "    left_panel = VBox([model_dropdown, interaction_mode], layout=Layout(margin=\"0px 20px 10px 0px\"))\n",
    "    right_panel = VBox([temperature_slider, top_p_slider, top_k_slider, num_beams_slider, repetition_penalty_slider],\n",
    "                       layout=Layout(margin=\"0px 0px 10px 20px\"))\n",
    "    user_input_widgets = HBox([left_panel, right_panel], layout=Layout(margin=\"0px 50px 10px 0px\"))\n",
    "    start_button = Button(description=\"Start Interaction!\")\n",
    "    start_button.layout.margin = '0 auto'\n",
    "    display(user_input_widgets)\n",
    "    display(start_button)\n",
    "    display(out)\n",
    "    \n",
    "    def on_start(button):\n",
    "        out.clear_output()\n",
    "        with out:\n",
    "            print(\"\\nSetting up the model, please wait...\")\n",
    "        #out.clear_output()\n",
    "        model_choice = model_dropdown.value\n",
    "        with_context = interaction_mode.value == interaction_modes[0]\n",
    "        temperature = temperature_slider.value\n",
    "        top_p = top_p_slider.value\n",
    "        top_k = top_k_slider.value\n",
    "        num_beams = num_beams_slider.value\n",
    "        repetition_penalty = repetition_penalty_slider.value\n",
    "        \n",
    "\n",
    "        bot = ChatBotModel(model_id_or_path=model_choice)\n",
    "        bot.warmup_model(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "        )\n",
    "        \n",
    "        with out:\n",
    "            print(\"Ready!\")\n",
    "            print(\"\\nNote: This is a demonstration using pretrained models which were not fine-tuned for chat.\\n\")      \n",
    "        try:\n",
    "            with out:\n",
    "                bot.interact(\n",
    "                    with_context=with_context,\n",
    "                    out=out,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    num_beams=num_beams,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            with out:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    start_button.on_click(on_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7322e1-abae-4a2a-9381-453dc0cf0c23",
   "metadata": {},
   "source": [
    "---\n",
    "**Let's Dive In and Have Some Fun with LLM Models!**\n",
    "\n",
    "Ready for a playful interaction with some interesting LLM models? The interface below lets you choose from different models and settings. Just select your preferences, click the \"Start Interaction!\" button, and you're ready to chat.\n",
    "\n",
    "You can ask questions, make statements, or simply explore how the model responds to different inputs. It's a friendly way to get acquainted with AI and see what it has to say.\n",
    "\n",
    "Remember, this is all in good fun, and the models are here to engage with you. So go ahead, start a conversation, and enjoy the interaction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b754474c-8a98-4d55-bb69-cd98a8c209e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deece7eddb9441de9f2edc95b3b1a009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Dropdown(description='Model:', options=('openlm-research/open_llama_3b_v2', 'Wri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45305bced8e64122980de99a99d9dd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Interaction!', layout=Layout(margin='0 auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb856e739aac4fa1901fc5524ea7f45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d5473-e7fe-4de0-a8c0-f49ba924f35d",
   "metadata": {},
   "source": [
    "## Language Models Disclaimer and Information\n",
    "\n",
    "### Camel-5B\n",
    "- **Model card:** [Camel-5B](https://huggingface.co/Writer/camel-5b-hf)\n",
    "- **License:** Apache 2.0\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @misc{Camel,\n",
    "      author = {Writer Engineering team},\n",
    "      title = {{Camel-5B InstructGPT}},\n",
    "      howpublished = {\\url{https://dev.writer.com}},\n",
    "      year = 2023,\n",
    "      month = April \n",
    "    }\n",
    "    ```\n",
    "\n",
    "### OpenLLaMA 3b v2\n",
    "- **Model card:** [OpenLLaMA 3b v2](https://huggingface.co/openlm-research/open_llama_3b_v2)\n",
    "- **License:** Apache 2.0\n",
    "- **References:**\n",
    "    ```bibtex\n",
    "    @software{openlm2023openllama,\n",
    "      author = {Geng, Xinyang and Liu, Hao},\n",
    "      title = {OpenLLaMA: An Open Reproduction of LLaMA},\n",
    "      month = May,\n",
    "      year = 2023,\n",
    "      url = {https://github.com/openlm-research/open_llama}\n",
    "    }\n",
    "    @software{together2023redpajama,\n",
    "      author = {Together Computer},\n",
    "      title = {RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset},\n",
    "      month = April,\n",
    "      year = 2023,\n",
    "      url = {https://github.com/togethercomputer/RedPajama-Data}\n",
    "    }\n",
    "    @article{touvron2023llama,\n",
    "      title={Llama: Open and efficient foundation language models},\n",
    "      author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},\n",
    "      journal={arXiv preprint arXiv:2302.13971},\n",
    "      year={2023}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "### Disclaimer for Using Large Language Models\n",
    "\n",
    "Please be aware that while Large Language Models like Camel-5B and OpenLLaMA 3b v2 are powerful tools for text generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It's advisable to carefully review the generated text and consider the context and application in which you are using these models.\n",
    "\n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c25c0c-d37a-499b-a551-8048cb54a011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
