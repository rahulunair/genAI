{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8b40b326-4549-4b2b-8ce1-a453fbaa7a19",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0\n",
    "Copyright (c) 2023, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d173d6a-a86d-441a-b36a-efed57814310",
   "metadata": {},
   "source": [
    "---\n",
    "**Simple LLM Inference: Playing with Language Models on Intel® Data Center Max Series GPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f2e84-54e8-4c1d-bfec-302f9dff577d",
   "metadata": {},
   "source": [
    "Hello and welcome! Are you curious about how computers understand and generate human-like text? Do you want to play around with text generation without getting too technical? Then you've come to the right place.\n",
    "\n",
    "Large Language Models (LLMs) have a wide range of applications, but they can also be fun to experiment with. Here, we'll use some simple pre-trained models to explore text generation interactively.\n",
    "\n",
    "Powered by Intel® Data Center GPU Max 1100s, this notebook provides a hands-on experience that doesn't require deep technical knowledge. Whether you're a student, writer, educator, or just curious about AI, this guide is designed for you.\n",
    "\n",
    "Ready to try it out? Let's set up our environment and start exploring the world of text generation with LLMs!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0baad4",
   "metadata": {},
   "source": [
    "# Setup\n",
    "1) Uncomment below cell and run to create new environment.\n",
    "2) To select new environment/kernel in Jupyter, select \"simple_llm_env\" in top-right dropdown of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5295d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export ENV_NAME=\"simple_llm_env\"\n",
    "export PATH=/opt/intel/miniforge3/condabin:$PATH # Specify conda path, can assume we have this on Intel Tiber Developer Cloud . \n",
    "\n",
    "conda create -n $ENV_NAME python=3.10 -y  # Some packages can be particular to python version.\n",
    "\n",
    "# Dependencies\n",
    "conda run -n $ENV_NAME pip install\\\n",
    " ipykernel==6.29.5\\\n",
    " ipywidgets==8.1.5\\\n",
    " pandas==2.2.2\\\n",
    " pytz\\\n",
    " numpy==1.26.4\\\n",
    " transformers==4.35.2  # Needed to upgrade from 4.32.1 to work\n",
    "\n",
    "# XPU dependencies\n",
    "conda run -n $ENV_NAME pip install\\\n",
    " torch==2.1.0.post2\\\n",
    " torchvision==0.16.0.post2\\\n",
    " torchaudio==2.1.0.post2\\\n",
    " oneccl_bind_pt==2.1.300+xpu\\\n",
    " intel-extension-for-pytorch==2.1.30+xpu  --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\\\n",
    "\n",
    "# To make environment available to Jupyter Kernel. Please select in dropdown (top right dropdown)\n",
    "conda run -n $ENV_NAME python -m ipykernel install --user --name=$ENV_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc93ee-117e-4d10-a97c-c26429b38159",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "from ipywidgets import VBox, HBox, Button, Dropdown, IntSlider, FloatSlider, Text, Output, Label, Layout\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HTML\n",
    "\n",
    "\n",
    "# random seed\n",
    "if torch.xpu.is_available():\n",
    "    seed = 88\n",
    "    random.seed(seed)\n",
    "    torch.xpu.manual_seed(seed)\n",
    "    torch.xpu.manual_seed_all(seed)\n",
    "\n",
    "def select_device(preferred_device=None):\n",
    "    \"\"\"\n",
    "    Selects the best available XPU device or the preferred device if specified.\n",
    "\n",
    "    Args:\n",
    "        preferred_device (str, optional): Preferred device string (e.g., \"cpu\", \"xpu\", \"xpu:0\", \"xpu:1\", etc.). If None, a random available XPU device will be selected or CPU if no XPU devices are available.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The selected device object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if preferred_device and preferred_device.startswith(\"cpu\"):\n",
    "            print(\"Using CPU.\")\n",
    "            return torch.device(\"cpu\")\n",
    "        if preferred_device and preferred_device.startswith(\"xpu\"):\n",
    "            if preferred_device == \"xpu\" or (\n",
    "                \":\" in preferred_device\n",
    "                and int(preferred_device.split(\":\")[1]) >= torch.xpu.device_count()\n",
    "            ):\n",
    "                preferred_device = (\n",
    "                    None  # Handle as if no preferred device was specified\n",
    "                )\n",
    "            else:\n",
    "                device = torch.device(preferred_device)\n",
    "                if device.type == \"xpu\" and device.index < torch.xpu.device_count():\n",
    "                    vram_used = torch.xpu.memory_allocated(device) / (\n",
    "                        1024**2\n",
    "                    )  # In MB\n",
    "                    print(\n",
    "                        f\"Using preferred device: {device}, VRAM used: {vram_used:.2f} MB\"\n",
    "                    )\n",
    "                    return device\n",
    "\n",
    "        if torch.xpu.is_available():\n",
    "            device_id = random.choice(\n",
    "                range(torch.xpu.device_count())\n",
    "            )  # Select a random available XPU device\n",
    "            device = torch.device(f\"xpu:{device_id}\")\n",
    "            vram_used = torch.xpu.memory_allocated(device) / (1024**2)  # In MB\n",
    "            print(f\"Selected device: {device}, VRAM used: {vram_used:.2f} MB\")\n",
    "            return device\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while selecting the device: {e}\")\n",
    "    print(\"No XPU devices available or preferred device not found. Using CPU.\")\n",
    "    return torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416381a3-6293-4701-a8fa-76f1402037dc",
   "metadata": {},
   "source": [
    "---\n",
    "**A Glimpse Into Text Generation with Language Models**\n",
    "\n",
    "If you're intrigued by how machines can generate human-like text, let's take a closer look at the underlying code. Even if you're not technically inclined, this section will provide a high-level understanding of how it all works.:\n",
    "\n",
    "- **Class Definition**: The `ChatBotModel` class is the core of our text generation. It handles the setup, optimization, and interaction with the LLM (Large Language Model).\n",
    "\n",
    "- **Initialization**: When you create an instance of this class, you can specify the model's path, the device to run on (defaulting to Intel's \"xpu\" device if available), and the data type. There's also an option to optimize the model for Intel GPUs using Intel Extension For PyTorch (IPEX).\n",
    "\n",
    "- **Input Preparation**: The `prepare_input` method ensures that the input doesn't exceed the maximum length and combines the previous text with the user input, if required.\n",
    "\n",
    "- **Output Generation**: The `gen_output` method takes the prepared input and several parameters controlling the generation process, like temperature, top_p, top_k, etc., and produces the text response.\n",
    "\n",
    "- **Warm-up**: Before the main interactions, the `warmup_model` method helps in \"warming up\" the model to make subsequent runs faster.\n",
    "\n",
    "- **Text Processing**: Several methods like `unique_sentences`, `remove_repetitions`, and `extract_bot_response` handle the text processing to ensure the generated text is readable and free from repetitions or unnecessary echoes.\n",
    "\n",
    "Feel free to explore the code and play around with different parameters. Remember, this is a simple and interactive way to experiment with text generation. It's not a cutting-edge chatbot, but rather a playful tool to engage with language models. Enjoy the journey into the world of LLMs, using Intel® Data Center GPU Max 1100s!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f29e9-cd41-4605-aafc-f5aaaf440469",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_CACHE_PATH = \"/home/common/data/Big_Data/GenAI/llm_models\"\n",
    "class ChatBotModel:\n",
    "    \"\"\"\n",
    "    ChatBotModel is a class for generating responses based on text prompts using a pretrained model.\n",
    "\n",
    "    Attributes:\n",
    "    - device: The device to run the model on. Default is \"xpu\" if available, otherwise \"cpu\".\n",
    "    - model: The loaded model for text generation.\n",
    "    - tokenizer: The loaded tokenizer for the model.\n",
    "    - torch_dtype: The data type to use in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path: str = \"openlm-research/open_llama_3b_v2\",  # \"Writer/camel-5b-hf\",\n",
    "        torch_dtype: torch.dtype = torch.bfloat16,\n",
    "        optimize: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        The initializer for ChatBotModel class.\n",
    "\n",
    "        Parameters:\n",
    "        - model_id_or_path: The identifier or path of the pretrained model.\n",
    "        - torch_dtype: The data type to use in the model. Default is torch.bfloat16.\n",
    "        - optimize: If True, ipex is used to optimize the model\n",
    "        \"\"\"\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.device = select_device(\"xpu\")\n",
    "        self.model_id_or_path = model_id_or_path\n",
    "        local_model_id = self.model_id_or_path.replace(\"/\", \"--\")\n",
    "        local_model_path = os.path.join(MODEL_CACHE_PATH, local_model_id)\n",
    "\n",
    "        if (\n",
    "            self.device == self.device.startswith(\"xpu\")\n",
    "            if isinstance(self.device, str)\n",
    "            else self.device.type == \"xpu\"\n",
    "        ):\n",
    "\n",
    "            self.autocast = torch.xpu.amp.autocast\n",
    "        else:\n",
    "            self.autocast = torch.cpu.amp.autocast\n",
    "        self.torch_dtype = torch_dtype\n",
    "        try:\n",
    "            if \"llama\" in model_id_or_path:\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(local_model_path)\n",
    "                self.model = (\n",
    "                    LlamaForCausalLM.from_pretrained(\n",
    "                        local_model_path,\n",
    "                        low_cpu_mem_usage=True,\n",
    "                        torch_dtype=self.torch_dtype,\n",
    "                    )\n",
    "                    .to(self.device)\n",
    "                    .eval()\n",
    "                )\n",
    "            else:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    local_model_path, trust_remote_code=True\n",
    "                )\n",
    "                self.model = (\n",
    "                    AutoModelForCausalLM.from_pretrained(\n",
    "                        local_model_path,\n",
    "                        low_cpu_mem_usage=True,\n",
    "                        trust_remote_code=True,\n",
    "                        torch_dtype=self.torch_dtype,\n",
    "                    )\n",
    "                    .to(self.device)\n",
    "                    .eval()\n",
    "                )\n",
    "        except (OSError, ValueError, EnvironmentError) as e:\n",
    "            logging.info(\n",
    "                f\"Tokenizer / model not found locally. Downloading tokenizer / model for {self.model_id_or_path} to cache...: {e}\"\n",
    "            )\n",
    "            if \"llama\" in model_id_or_path:\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(self.model_id_or_path)\n",
    "                self.model = (\n",
    "                    LlamaForCausalLM.from_pretrained(\n",
    "                        self.model_id_or_path,\n",
    "                        low_cpu_mem_usage=True,\n",
    "                        torch_dtype=self.torch_dtype,\n",
    "                    )\n",
    "                    .to(self.device)\n",
    "                    .eval()\n",
    "                )\n",
    "            else:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    self.model_id_or_path, trust_remote_code=True\n",
    "                )\n",
    "                self.model = (\n",
    "                    AutoModelForCausalLM.from_pretrained(\n",
    "                        self.model_id_or_path,\n",
    "                        low_cpu_mem_usage=True,\n",
    "                        trust_remote_code=True,\n",
    "                        torch_dtype=self.torch_dtype,\n",
    "                    )\n",
    "                    .to(self.device)\n",
    "                    .eval()\n",
    "                )\n",
    "            \n",
    "        self.max_length = 256\n",
    "\n",
    "        if optimize:\n",
    "            if hasattr(ipex, \"optimize_transformers\"):\n",
    "                try:\n",
    "                    ipex.optimize_transformers(self.model, dtype=self.torch_dtype)\n",
    "                except:\n",
    "                    ipex.optimize(self.model, dtype=self.torch_dtype)\n",
    "            else:\n",
    "                ipex.optimize(self.model, dtype=self.torch_dtype)\n",
    "\n",
    "    def prepare_input(self, previous_text, user_input):\n",
    "        \"\"\"Prepare the input for the model, ensuring it doesn't exceed the maximum length.\"\"\"\n",
    "        response_buffer = 100\n",
    "        user_input = (\n",
    "            \"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{user_input}\\n\\n### Response:\")\n",
    "        combined_text = previous_text + \"\\nUser: \" + user_input + \"\\nBot: \"\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            combined_text, return_tensors=\"pt\", truncation=False\n",
    "        )\n",
    "        adjusted_max_length = self.max_length - response_buffer\n",
    "        if input_ids.shape[1] > adjusted_max_length:\n",
    "            input_ids = input_ids[:, -adjusted_max_length:]\n",
    "        return input_ids.to(device=self.device)\n",
    "\n",
    "    def gen_output(\n",
    "        self, input_ids, temperature, top_p, top_k, num_beams, repetition_penalty\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate the output text based on the given input IDs and generation parameters.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): The input tensor containing token IDs.\n",
    "            temperature (float): The temperature for controlling randomness in Boltzmann distribution.\n",
    "                                Higher values increase randomness, lower values make the generation more deterministic.\n",
    "            top_p (float): The cumulative distribution function (CDF) threshold for Nucleus Sampling.\n",
    "                           Helps in controlling the trade-off between randomness and diversity.\n",
    "            top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "            num_beams (int): The number of beams for beam search. Controls the breadth of the search.\n",
    "            repetition_penalty (float): The penalty applied for repeating tokens.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The generated output tensor.\n",
    "        \"\"\"\n",
    "        print(f\"Using max length: {self.max_length}\")\n",
    "        with self.autocast(\n",
    "            enabled=True if self.torch_dtype != torch.float32 else False,\n",
    "            dtype=self.torch_dtype,\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                output = self.model.generate(\n",
    "                    input_ids,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    max_length=self.max_length,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    num_beams=num_beams,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                )\n",
    "                return output\n",
    "\n",
    "    def warmup_model(\n",
    "        self, temperature, top_p, top_k, num_beams, repetition_penalty\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Warms up the model by generating a sample response.\n",
    "        \"\"\"\n",
    "        sample_prompt = \"\"\"A dialog, where User interacts with a helpful Bot.\n",
    "        AI is helpful, kind, obedient, honest, and knows its own limits.\n",
    "        User: Hello, Bot.\n",
    "        Bot: Hello! How can I assist you today?\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer(sample_prompt, return_tensors=\"pt\").input_ids.to(\n",
    "            device=self.device\n",
    "        )\n",
    "        _ = self.gen_output(\n",
    "            input_ids,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "        )\n",
    "\n",
    "    def strip_response(self, generated_text):\n",
    "        \"\"\"Remove ### Response: from string if exists.\"\"\"\n",
    "        match = re.search(r'### Response:(.*)', generated_text, re.S)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "        else:\n",
    "            return generated_text\n",
    "        \n",
    "    def unique_sentences(self, text: str) -> str:\n",
    "        sentences = text.split(\". \")\n",
    "        if sentences[-1] and sentences[-1][-1] != \".\":\n",
    "            sentences = sentences[:-1]\n",
    "        sentences = set(sentences)\n",
    "        return \". \".join(sentences) + \".\" if sentences else \"\"\n",
    "\n",
    "    def remove_repetitions(self, text: str, user_input: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove repetitive sentences or phrases from the generated text and avoid echoing user's input.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text with potential repetitions.\n",
    "            user_input (str): The user's original input to check against echoing.\n",
    "\n",
    "        Returns:\n",
    "            str: The processed text with repetitions and echoes removed.\n",
    "        \"\"\"\n",
    "        text = re.sub(re.escape(user_input), \"\", text, count=1).strip()\n",
    "        text = self.unique_sentences(text)\n",
    "        return text\n",
    "\n",
    "    def extract_bot_response(self, generated_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the first response starting with \"Bot:\" from the generated text.\n",
    "\n",
    "        Args:\n",
    "            generated_text (str): The full generated text from the model.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted response starting with \"Bot:\".\n",
    "        \"\"\"\n",
    "        prefix = \"Bot:\"\n",
    "        generated_text = generated_text.replace(\"\\n\", \". \")\n",
    "        bot_response_start = generated_text.find(prefix)\n",
    "        if bot_response_start != -1:\n",
    "            response_start = bot_response_start + len(prefix)\n",
    "            end_of_response = generated_text.find(\"\\n\", response_start)\n",
    "            if end_of_response != -1:\n",
    "                return generated_text[response_start:end_of_response].strip()\n",
    "            else:\n",
    "                return generated_text[response_start:].strip()\n",
    "        return re.sub(r'^[^a-zA-Z0-9]+', '', generated_text)\n",
    "\n",
    "    def interact(\n",
    "        self,\n",
    "        out: Output,  # Output widget to display the conversation\n",
    "        with_context: bool = True,\n",
    "        temperature: float = 0.10,\n",
    "        top_p: float = 0.95,\n",
    "        top_k: int = 40,\n",
    "        num_beams: int = 3,\n",
    "        repetition_penalty: float = 1.80,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Handle the chat loop where the user provides input and receives a model-generated response.\n",
    "\n",
    "        Args:\n",
    "            with_context (bool): Whether to consider previous interactions in the session. Default is True.\n",
    "            temperature (float): The temperature for controlling randomness in Boltzmann distribution.\n",
    "                                 Higher values increase randomness, lower values make the generation more deterministic.\n",
    "            top_p (float): The cumulative distribution function (CDF) threshold for Nucleus Sampling.\n",
    "                           Helps in controlling the trade-off between randomness and diversity.\n",
    "            top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "            num_beams (int): The number of beams for beam search. Controls the breadth of the search.\n",
    "            repetition_penalty (float): The penalty applied for repeating tokens.\n",
    "            \"\"\"\n",
    "        previous_text = \"\"\n",
    "    \n",
    "        def display_user_input_widgets():\n",
    "            default_color = \"\\033[0m\"\n",
    "            user_color, user_icon = \"\\033[94m\", \"😀 \"\n",
    "            bot_color, bot_icon = \"\\033[92m\", \"🤖 \"\n",
    "            user_input_widget = Text(placeholder=\"Type your message here...\", layout=Layout(width='80%'))\n",
    "            send_button = Button(description=\"Send\", button_style = \"primary\", layout=Layout(width='10%'))\n",
    "            chat_spin = HTML(value = \"\")\n",
    "            spin_style = \"\"\"\n",
    "            <div class=\"loader\"></div>\n",
    "            <style>\n",
    "            .loader {\n",
    "              border: 5px solid #f3f3f3;\n",
    "              border-radius: 50%;\n",
    "              border-top: 5px solid #3498db;\n",
    "              width: 8px;\n",
    "              height: 8px;\n",
    "              animation: spin 3s linear infinite;\n",
    "            }\n",
    "            @keyframes spin {\n",
    "              0% { transform: rotate(0deg); }\n",
    "              100% { transform: rotate(360deg); }\n",
    "            }\n",
    "            </style>\n",
    "            \"\"\"\n",
    "            display(HBox([chat_spin, user_input_widget, send_button, ]))\n",
    "            \n",
    "            def on_send(button):\n",
    "                nonlocal previous_text\n",
    "                send_button.button_style = \"warning\"\n",
    "                chat_spin.value = spin_style\n",
    "                orig_input = \"\"\n",
    "                user_input = user_input_widget.value\n",
    "                with out:\n",
    "                    print(f\" {user_color}{user_icon}You: {user_input}{default_color}\")\n",
    "                if user_input.lower() == \"exit\":\n",
    "                    return\n",
    "                if \"camel\" in self.model_id_or_path:\n",
    "                        orig_input = user_input\n",
    "                        user_input = (\n",
    "                            \"Below is an instruction that describes a task. \"\n",
    "                            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "                            f\"### Instruction:\\n{user_input}\\n\\n### Response:\")\n",
    "                if with_context:\n",
    "                    self.max_length = 256\n",
    "                    input_ids = self.prepare_input(previous_text, user_input)\n",
    "                else:\n",
    "                    self.max_length = 96\n",
    "                    input_ids = self.tokenizer.encode(user_input, return_tensors=\"pt\").to(self.device)\n",
    "    \n",
    "                output_ids = self.gen_output(\n",
    "                    input_ids,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    num_beams=num_beams,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                )\n",
    "                generated_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                generated_text = self.strip_response(generated_text)\n",
    "                generated_text = self.extract_bot_response(generated_text)\n",
    "                generated_text = self.remove_repetitions(generated_text, user_input)\n",
    "                send_button.button_style = \"success\"\n",
    "                chat_spin.value = \"\"\n",
    "\n",
    "                with out:\n",
    "                    if orig_input:\n",
    "                        user_input = orig_input\n",
    "                    print(f\" {bot_color}{bot_icon}Bot: {generated_text}{default_color}\")    \n",
    "                if with_context:\n",
    "                    previous_text += \"\\nUser: \" + user_input + \"\\nBot: \" + generated_text\n",
    "                user_input_widget.value = \"\" \n",
    "                display_user_input_widgets()\n",
    "            send_button.on_click(on_send)\n",
    "        display_user_input_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a61ad2-5155-4b07-9f10-d9d18f252e8f",
   "metadata": {},
   "source": [
    "---\n",
    "**Setting Up the Interactive Text Generation Interface**\n",
    "\n",
    "In the next section, we'll create an interactive text generation interface right here in this notebook. This will enable you to select a model, provide a prompt, and tweak various parameters without touching the code itself.\n",
    "\n",
    "- **Model Selection**: Choose from available pre-trained models or enter a custom model from the HuggingFace Hub.\n",
    "- **Interaction Mode**: Decide whether to interact with or without context, allowing the model to remember previous interactions or treat each input independently.\n",
    "- **Temperature**: Adjust this parameter to control the randomness in text generation. Higher values increase creativity; lower values make the generation more deterministic.\n",
    "- **Top_p, Top_k**: Play with these parameters to influence the diversity and quality of the generated text.\n",
    "- **Number of Beams**: Control the breadth of the search in text generation.\n",
    "- **Repetition Penalty**: Modify this to prevent or allow repeated phrases and sentences.\n",
    "\n",
    "Once you've set your preferences, you can start the interaction and even reset or reload the model to try different settings. Let's set this up and explore the playful world of text generation using Intel® Data Center GPU Max 1100s!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42264502-ffdd-47e1-af1e-f7407c4b365f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_cache = {}\n",
    "\n",
    "from ipywidgets import HTML\n",
    "def interact_with_llm():\n",
    "    models = [\"Writer/camel-5b-hf\", \n",
    "              \"openlm-research/open_llama_3b_v2\",\n",
    "              \"Intel/neural-chat-7b-v3\", \n",
    "              \"Intel/neural-chat-7b-v3-1\", # https://huggingface.co/Intel/neural-chat-7b-v3-1 - checkout the prompting template on the site to get better response.\n",
    "              \"HuggingFaceH4/zephyr-7b-beta\", \n",
    "              \"tiiuae/falcon-7b\"\n",
    "             ]\n",
    "    interaction_modes = [\"Interact with context\", \"Interact without context\"]\n",
    "    model_dropdown = Dropdown(options=models, value=models[0], description=\"Model:\")\n",
    "    interaction_mode = Dropdown(options=interaction_modes, value=interaction_modes[1], description=\"Interaction:\")\n",
    "    temperature_slider = FloatSlider(value=0.71, min=0, max=1, step=0.01, description=\"Temperature:\")\n",
    "    top_p_slider = FloatSlider(value=0.95, min=0, max=1, step=0.01, description=\"Top P:\")\n",
    "    top_k_slider = IntSlider(value=40, min=0, max=100, step=1, description=\"Top K:\")\n",
    "    num_beams_slider = IntSlider(value=3, min=1, max=10, step=1, description=\"Num Beams:\")\n",
    "    repetition_penalty_slider = FloatSlider(value=1.80, min=0, max=2, step=0.1, description=\"Rep Penalty:\")\n",
    "    \n",
    "    out = Output()    \n",
    "    left_panel = VBox([model_dropdown, interaction_mode], layout=Layout(margin=\"0px 20px 10px 0px\"))\n",
    "    right_panel = VBox([temperature_slider, top_p_slider, top_k_slider, num_beams_slider, repetition_penalty_slider],\n",
    "                       layout=Layout(margin=\"0px 0px 10px 20px\"))\n",
    "    user_input_widgets = HBox([left_panel, right_panel], layout=Layout(margin=\"0px 50px 10px 0px\"))\n",
    "    spinner = HTML(value=\"\")\n",
    "    start_button = Button(description=\"Start Interaction!\", button_style=\"primary\")\n",
    "    start_button_spinner = HBox([start_button, spinner])\n",
    "    start_button_spinner.layout.margin = '0 auto'\n",
    "    display(user_input_widgets)\n",
    "    display(start_button_spinner)\n",
    "    display(out)\n",
    "    \n",
    "    def on_start(button):\n",
    "        start_button.button_style = \"warning\"\n",
    "        start_button.description = \"Loading...\"\n",
    "        spinner.value = \"\"\"\n",
    "        <div class=\"loader\"></div>\n",
    "        <style>\n",
    "        .loader {\n",
    "          border: 5px solid #f3f3f3;\n",
    "          border-radius: 50%;\n",
    "          border-top: 5px solid #3498db;\n",
    "          width: 16px;\n",
    "          height: 16px;\n",
    "          animation: spin 3s linear infinite;\n",
    "        }\n",
    "        @keyframes spin {\n",
    "          0% { transform: rotate(0deg); }\n",
    "          100% { transform: rotate(360deg); }\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\"\n",
    "        out.clear_output()\n",
    "        with out:\n",
    "            print(\"\\nSetting up the model, please wait...\")\n",
    "            #out.clear_output()\n",
    "            model_choice = model_dropdown.value\n",
    "            with_context = interaction_mode.value == interaction_modes[0]\n",
    "            temperature = temperature_slider.value\n",
    "            top_p = top_p_slider.value\n",
    "            top_k = top_k_slider.value\n",
    "            num_beams = num_beams_slider.value\n",
    "            repetition_penalty = repetition_penalty_slider.value\n",
    "            model_key = (model_choice, \"xpu\")\n",
    "            if model_key not in model_cache:\n",
    "                print(f\"Caching Chatbot {model_key}\")\n",
    "                model_cache[model_key] = ChatBotModel(model_id_or_path=model_choice)\n",
    "            bot = model_cache[model_key]\n",
    "            \n",
    "            start_button.button_style = \"success\"\n",
    "            start_button.description = \"Refresh\"\n",
    "            spinner.value = \"\"\n",
    "            print(\"Ready!\")\n",
    "            print(\"\\nNote: This is a demonstration using pretrained models which were not fine-tuned for chat.\")\n",
    "            print(\"If the bot doesn't respond, try clicking on refresh.\\n\")\n",
    "            try:\n",
    "                bot.interact(\n",
    "                    with_context=with_context,\n",
    "                    out=out,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    num_beams=num_beams,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                with out:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "\n",
    "    start_button.on_click(on_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7322e1-abae-4a2a-9381-453dc0cf0c23",
   "metadata": {},
   "source": [
    "---\n",
    "**Let's Dive In and Have Some Fun with LLM Models!**\n",
    "\n",
    "Ready for a playful interaction with some interesting LLM models? The interface below lets you choose from different models and settings. Just select your preferences, click the \"Start Interaction!\" button, and you're ready to chat.\n",
    "\n",
    "You can ask questions, make statements, or simply explore how the model responds to different inputs. It's a friendly way to get acquainted with AI and see what it has to say.\n",
    "\n",
    "Remember, this is all in good fun, and the models are here to engage with you. So go ahead, start a conversation, and enjoy the interaction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754474c-8a98-4d55-bb69-cd98a8c209e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interact_with_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d5473-e7fe-4de0-a8c0-f49ba924f35d",
   "metadata": {},
   "source": [
    "## Language Models Disclaimer and Information\n",
    "\n",
    "### Camel-5B\n",
    "- **Model card:** [Camel-5B](https://huggingface.co/Writer/camel-5b-hf)\n",
    "- **License:** Apache 2.0\n",
    "- **Reference:**\n",
    "    ```bibtex\n",
    "    @misc{Camel,\n",
    "      author = {Writer Engineering team},\n",
    "      title = {{Camel-5B InstructGPT}},\n",
    "      howpublished = {\\url{https://dev.writer.com}},\n",
    "      year = 2023,\n",
    "      month = April \n",
    "    }\n",
    "    ```\n",
    "\n",
    "### OpenLLaMA 3b v2\n",
    "- **Model card:** [OpenLLaMA 3b v2](https://huggingface.co/openlm-research/open_llama_3b_v2)\n",
    "- **License:** Apache 2.0\n",
    "- **References:**\n",
    "    ```bibtex\n",
    "    @software{openlm2023openllama,\n",
    "      author = {Geng, Xinyang and Liu, Hao},\n",
    "      title = {OpenLLaMA: An Open Reproduction of LLaMA},\n",
    "      month = May,\n",
    "      year = 2023,\n",
    "      url = {https://github.com/openlm-research/open_llama}\n",
    "    }\n",
    "    @software{together2023redpajama,\n",
    "      author = {Together Computer},\n",
    "      title = {RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset},\n",
    "      month = April,\n",
    "      year = 2023,\n",
    "      url = {https://github.com/togethercomputer/RedPajama-Data}\n",
    "    }\n",
    "    @article{touvron2023llama,\n",
    "      title={Llama: Open and efficient foundation language models},\n",
    "      author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},\n",
    "      journal={arXiv preprint arXiv:2302.13971},\n",
    "      year={2023}\n",
    "    }\n",
    "    ```\n",
    "### Falcon 7B\n",
    "\n",
    "- **Model card:** [Falcon 7B](https://huggingface.co/tiiuae/falcon-7b)\n",
    "- **License:** Apache 2.0\n",
    "- **References:**\n",
    "  ```bibtex\n",
    "    @article{falcon40b,\n",
    "      title = {{Falcon-40B}: an open large language model with state-of-the-art performance},\n",
    "      author = {Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malrtic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n",
    "      year={2023}\n",
    "      }\n",
    "  ```\n",
    "### Zephyr 7B\n",
    "\n",
    "- **Model card:** [Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)\n",
    "- **License:** MIT\n",
    "- **References:**\n",
    "  ```bibtex\n",
    "    @misc{alignment_handboox2023,\n",
    "        author = {Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Alexander M. Rush and Thomas Wolf},\n",
    "        title = {The Alignment Handbook},\n",
    "        year = {2023},\n",
    "        publisher = {GitHub},\n",
    "        journal = {GitHub repository},\n",
    "        howpublished = {\\url{https://github.com/huggingface/alignment-handbook}}\n",
    "        }\n",
    "    ```\n",
    "### Neural Chat 7b\n",
    "- **Model card:** [Neural Chat](https://huggingface.co/Intel/neural-chat-7b-v3)\n",
    "- **License:** Apache 2.0\n",
    "\n",
    "### Disclaimer for Using Large Language Models\n",
    "\n",
    "Please be aware that while Large Language Models like Camel-5B and OpenLLaMA 3b v2 are powerful tools for text generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It's advisable to carefully review the generated text and consider the context and application in which you are using these models.\n",
    "\n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above.\n",
    "\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    " \n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intel’s provision of these resources does not expand or otherwise alter Intel’s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
